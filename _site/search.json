[
  {
    "objectID": "posts/soc-prediction/index.html",
    "href": "posts/soc-prediction/index.html",
    "title": "Soil Organic Carbon Estimation with Machine Learning",
    "section": "",
    "text": "This was one of the first projects I worked on at Vertify. The task was to build a prediction model for soil organic carbon (SOC) for a region in India—on a very tight deadline. I kept it simple and focused on getting something working quickly, mainly because:\n\nThere wasn’t much time to experiment\nThe dataset was quite limited\n\nThe ground truth data came from soil samples collected across multiple Indian states. But there was a catch: the data had been gathered by different agencies, with no consistency in measurement or formatting. After some comparisons, we realized the data quality was pretty poor.\nA major part of the project became figuring out which datasets were usable. Each subset produced very different model behavior. And given the diversity in Indian landscapes, building a stable model was not straightforward.\nInitially, model accuracy was below 20%. Through several rounds of data cleaning, variable selection, and feature engineering, it gradually improved. The biggest gain came after incorporating geographic context as part of the input. In the end, we were able to reach about 80% accuracy.\nAll initial experiments were done using the Google Earth Engine Python API, and the final implementation was deployed directly in GEE.\nGitHub Repo:\nThe results were later presented to the Ministry of Agriculture in India."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "",
    "text": "Forest and Agricultural fires are a major threat to the environment and human life. They can cause severe damage to the environment, property, and human life. In 2019, the Amazon rainforest wildfires burned more than 906,000 hectares of forest. In 2020, the Australian wildfires burned more than 18 million hectares of land. In 2021, the California wildfires burned more than 1.6 million hectares of land.\nThe ability to predict the burned areas on satellite images can help us to better understand the impact of wildfires on the environment and human life. It can also help us to better manage the wildfires and reduce their impact on the environment and human life.\n\n\n\nThe shaded area in this satellite image shows the extent of the burned area from the Parnitha fire in Greece. The fire, which broke out on July 23, 2023, has burned over 10,000 hectares of forest and caused widespread damage.\n\n\nIn this notebook, we will use machine learning to predict the burned areas on Landsat 5 images. We will use python to prepare the data, train the model, and evaluate the model.Specifically, we will use Random Forest algorithm to predict the burned area on the image using scikit-learn. We will follow the study area from my co-authored paper with my colleagues at Ashoka University on comparing methods to detect burned area in Central India. The paper is published in Frontiers in Forests and Global Change and can be accessed here.\nThe post is divided into 5 parts: 1). Image acquisition 2) Data preparation, 3) Model training, 4) Model evaluation and 5) Prediction on the image\n\n\nLandsat is a joint programme of the USGS and NASA. Landsat satellites image the entire Earth’s surface at a 30-meter resolution about once every two weeks. The Latest instalment on the series is Landst 9 which became operational in 2021. We would be using image from Landsat 5 which has been running from 1984 to 2013, thus providing us accessibly and capability to inquire about burned area historically. The USGS produces data in 3 categories for each satellite (Tier 1, Tier 2 and R). We will be using landsat 5 collection 2:level 2:tier 1. Tier 1 (T1) data meets geometric and radiometric quality requirements.\nThis dataset contains atmospherically corrected surface reflectance and land surface temperature derived from the data produced by the Landsat TM sensor. These images contain 4 visible and near-infrared (VNIR) bands and 2 short-wave infrared (SWIR) bands processed to orthorectified surface reflectance, and one thermal infrared (TIR) band processed to orthorectified surface temperature. We will use all the bands except the thermal bands since the thermal band has different resolution (100m) than others.\nThere are a bunch of ways to download the data:\n\nGoogle Earth Engine\nUSGS Explorer\nAWS\nMicrosoft Planetary Computer\n\nIn this post, we will use planetary computer’s open access data catalogue to download the data. The data is stored in cloud optimised geotiff format which makes it easy to access and process."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#introduction",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#introduction",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "",
    "text": "Forest and Agricultural fires are a major threat to the environment and human life. They can cause severe damage to the environment, property, and human life. In 2019, the Amazon rainforest wildfires burned more than 906,000 hectares of forest. In 2020, the Australian wildfires burned more than 18 million hectares of land. In 2021, the California wildfires burned more than 1.6 million hectares of land.\nThe ability to predict the burned areas on satellite images can help us to better understand the impact of wildfires on the environment and human life. It can also help us to better manage the wildfires and reduce their impact on the environment and human life.\n\n\n\nThe shaded area in this satellite image shows the extent of the burned area from the Parnitha fire in Greece. The fire, which broke out on July 23, 2023, has burned over 10,000 hectares of forest and caused widespread damage.\n\n\nIn this notebook, we will use machine learning to predict the burned areas on Landsat 5 images. We will use python to prepare the data, train the model, and evaluate the model.Specifically, we will use Random Forest algorithm to predict the burned area on the image using scikit-learn. We will follow the study area from my co-authored paper with my colleagues at Ashoka University on comparing methods to detect burned area in Central India. The paper is published in Frontiers in Forests and Global Change and can be accessed here.\nThe post is divided into 5 parts: 1). Image acquisition 2) Data preparation, 3) Model training, 4) Model evaluation and 5) Prediction on the image\n\n\nLandsat is a joint programme of the USGS and NASA. Landsat satellites image the entire Earth’s surface at a 30-meter resolution about once every two weeks. The Latest instalment on the series is Landst 9 which became operational in 2021. We would be using image from Landsat 5 which has been running from 1984 to 2013, thus providing us accessibly and capability to inquire about burned area historically. The USGS produces data in 3 categories for each satellite (Tier 1, Tier 2 and R). We will be using landsat 5 collection 2:level 2:tier 1. Tier 1 (T1) data meets geometric and radiometric quality requirements.\nThis dataset contains atmospherically corrected surface reflectance and land surface temperature derived from the data produced by the Landsat TM sensor. These images contain 4 visible and near-infrared (VNIR) bands and 2 short-wave infrared (SWIR) bands processed to orthorectified surface reflectance, and one thermal infrared (TIR) band processed to orthorectified surface temperature. We will use all the bands except the thermal bands since the thermal band has different resolution (100m) than others.\nThere are a bunch of ways to download the data:\n\nGoogle Earth Engine\nUSGS Explorer\nAWS\nMicrosoft Planetary Computer\n\nIn this post, we will use planetary computer’s open access data catalogue to download the data. The data is stored in cloud optimised geotiff format which makes it easy to access and process."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#importing-libraries",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#importing-libraries",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Importing Libraries",
    "text": "Importing Libraries\nWe start by importing the necessary libraries.\n\n#  pystac for STAC queries\nimport pystac\nimport pystac_client\n\n\n#  to access data on the Planetary Computer platform\nimport planetary_computer\n\n# for reading raster data\nimport rasterio\n\n# for reading and manipulating vector data\nimport geopandas as gpd\n\n\n# y for working with arrays\nimport numpy as np\n\n#  for machine learning algorithms\nimport sklearn"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#downlaoding-the-image-from-planetary-computer",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#downlaoding-the-image-from-planetary-computer",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Downlaoding the Image from Planetary computer",
    "text": "Downlaoding the Image from Planetary computer\n\nPlanetary Computer is Microsoft’s equivalent of Google Earth Engine (GEE), but with some key differences. Unlike GEE, which is primarily based on a JavaScript platform, Planetary Computer offers Python (CPU and GPU) and R notebooks for computation. This provides users with the flexibility to work in their preferred programming language for geospatial analysis.\nWhat sets Planetary Computer apart is its public data repository and open access API, which supports querying data using SpatioTemporal Asset Catalogs (STAC). STAC is a specification that defines a standardised format for describing geospatial data. It enables machine-readable structures for efficient data querying and downloading from the repository.\nIf you want to learn more about STAC, you can check out their official documentation at STAC.\nIn our specific case, accessing the desired image from Planetary Computer is quite straightforward. Simply specify the common access point, collection, and image ID, and you can retrieve the image for further analysis and processing. This simplified data retrieval process allows us to focus on our analysis tasks without unnecessary complexities.\n\n\n\n# specify the api access point\napi_url = 'https://planetarycomputer.microsoft.com/api/stac/v1/'\n\n# image collection\ncollection = 'landsat-c2-l2'\n\n# image ID\nimage_id = 'LT05_L2SP_145044_20100428_02_T1'\n\n# access the catalogue and get our image assets\ncatalogue = pystac_client.Client.open(api_url)\ncollection = catalogue.get_collection(collection)\nitem = collection.get_item(image_id)\nitem = planetary_computer.sign(item)\n\n# call the assets\nassets = item.assets\n\n\n# print the bands\nfor keys, asset in assets.items():\n    print(f\"{keys}: {asset.title}\")\n\nqa: Surface Temperature Quality Assessment Band\nang: Angle Coefficients File\nred: Red Band\nblue: Blue Band\ndrad: Downwelled Radiance Band\nemis: Emissivity Band\nemsd: Emissivity Standard Deviation Band\nlwir: Surface Temperature Band\ntrad: Thermal Radiance Band\nurad: Upwelled Radiance Band\natran: Atmospheric Transmittance Band\ncdist: Cloud Distance Band\ngreen: Green Band\nnir08: Near Infrared Band 0.8\nswir16: Short-wave Infrared Band 1.6\nswir22: Short-wave Infrared Band 2.2\nmtl.txt: Product Metadata File (txt)\nmtl.xml: Product Metadata File (xml)\ncloud_qa: Cloud Quality Assessment Band\nmtl.json: Product Metadata File (json)\nqa_pixel: Pixel Quality Assessment Band\nqa_radsat: Radiometric Saturation and Dropped Pixel Quality Assessment Band\natmos_opacity: Atmospheric Opacity Band\ntilejson: TileJSON with default rendering\nrendered_preview: Rendered preview\n\n\nTo conduct our analysis, we carefully choose the bands we need from the provided list. We specify these bands as a list of strings, using them to access the download link for each specific band. Leveraging the power of the rasterio library, we read in the bands from their respective links. Then, employing the versatility of numpy, we cleverly stack the bands together, resulting in a cohesive and comprehensive single image for further analysis.\n\n# list of bands we need\nbands_list = [ 'red', 'green','blue', 'nir08', 'swir16', 'swir22' ]\n\n\n# extract the urls\nband_urls = [assets[band].href for band in bands_list] \n\n# we use the urls to read in the image\nraster_pc = [rasterio.open(url) for url in band_urls]\nbands_pc = [band.read(1) for band in raster_pc]\n\n# we stack the individual bands\n# we specify as xis 0 since the the individual raster shape is (bands, row, col) and we want to add the rasters through the same axis\nl5_pc = np.stack(bands_pc, axis = 0)\n\n# check the shape of stacked raster\nl5_pc.shape\n\n(6, 6901, 7811)\n\n\n\n#check the data range\n(l5_pc.min(), l5_pc.max())\n\n(0, 65535)\n\n\nWe observe that the image has 6 bands, and its shape is 6 x 6901 x 7811. Additionally, we identify the presence of nodata values, which are currently represented by 0. To handle this, we will replace these nodata values with NaN (Not-a-Number), ensuring they do not affect our subsequent calculations.\n\nScaling\nThe data ranges from 0 to 65535, as it is stored in a 16-bit format. To convert this data to surface reflectance, which ranges from 0 to 1, we will use a scale and offset factor. Scaling the data to surface reflectance offers several benefits:\nEasier Interpretation: Surface reflectance values are more intuitive and easier to interpret compared to raw DN (Digital Number) values.\nImproved Machine Learning: Machine learning algorithms often perform better with scaled data. By converting to surface reflectance, we enhance the compatibility and effectiveness of these algorithms.\nEnhanced visualisation: Surface reflectance values are visually appealing and easier to visualise, aiding in data exploration and result communication.\nBy performing the scaling process, we ensure that the data is appropriately transformed and ready for further analysis.\n\n# create a mask for all the nodata using the first band\nmask = np.ma.masked_equal(l5_pc,raster_pc[0].nodata)\n\n\n#  Create an empty numpy array to store the scaled values\nl5_scaled = np.zeros(shape= l5_pc.shape, dtype= rasterio.float64)\n\n\nmult = 0.0000275 # scale\nadd = -0.2 # offset\n\n\n# now that we have same mask for every band, we can directly scale\n# we use np.where to restrict operations on non-masked elements\n# at the same time we convert the zero's no NaN since anyway the scaled data will be in float64\n\nl5_scaled =  np.where(~mask.mask,l5_pc* mult + add, np.nan)\n\n# check the data range\nprint((np.nanmin(l5_scaled), np.nanmax(l5_scaled)))\n\n(-0.0774325, 1.6022125)\n\n\n\n# check the size after\n# size will increase since the dtype is float64\n\nprint(f\"data type and the size before conversion is: {l5_pc.dtype}, {round(l5_pc.nbytes/1024**3,2)} GB\")\nprint(f\"data type and the size after conversion is: {l5_scaled.dtype}, {round(l5_scaled.nbytes/1024**3,2)} GB\")      \n\n\ndata type and the size before conversion is: uint16, 0.6 GB\ndata type and the size after conversion is: float64, 2.41 GB\n\n\nWe notice a significant increase in the image size from 0.6 GB to 2.41 GB. This enlargement is a consequence of converting the data from a 16-bit integer to a 64-bit float format, providing us with more precise decimal points.\nNow that we have our image stacked, scaled, and the nodata values replaced with NaN, it’s time to save it for future use. To accomplish this task, we turn to the powerful rasterio library, which offers efficient functions specifically designed for handling raster data. With rasterio, we can effortlessly save our processed image to our preferred device, ensuring easy accessibility for subsequent analysis and visualisation.\nUsing rasterio, we can save the processed image to our desired location, ensuring that it is readily available for further analysis and visualisation when needed.\n\n\noutput_l5 = './data/145044_20090425_ms_pc.tif'\n# we change the data to float64 since we scaled it\n\nprofile = raster_pc[1].profile\nprofile.update({\n    'count':6,\n    'dtype': rasterio.float64,\n    'nodata': np.nan\n})\n\n# keep in mind - compression is `deflate`\n# hence reducing the image size ~2.5GB gets reduced to ~300MB.\n\nwith rasterio.open(output_l5, 'w', **profile) as dst:\n    dst.write(l5_scaled)"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#read-the-data",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#read-the-data",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Read the data",
    "text": "Read the data\nTo begin, we read the training points from a geojson file using the geopandas library. These points were manually annotated by visually inspecting the image. Our annotation process involved creating a combination of NIR, Red, and Green bands, which allows us to observe burned areas as purple in color. Admittedly, identifying pixel classes at a 30m resolution can be challenging. However, with some remote sensing knowledge and training, it becomes feasible.\nI would like to express my gratitude to my supervisor, Dr. Meghna, whose decades of experience in studying the area and expertise in fires have been invaluable. Her knowledge has guided us throughout this project. I would also like to thank my co-RA, Wajida, who has been a real saviour in the annotation process. She has an exceptional talent for interpreting raw satellite images, and her contributions have been truly outstanding. A big shout-out to her!\nIn our research paper, we had approximately 12 classes, including forest, burned forest, agriculture, burned agriculture, fallen leaves, agriculture with no crop, water, shadows, and more. The aim of having these diverse classes was to differentiate between burned areas in forests and agriculture while also assessing potential confounding factors related to fire. This approach helps us build a more robust model. However, for the sake of simplicity in our current analysis, we have reclassified them into just two classes.\n\nVector Data\nFor handling the vector data, we leverage the geopandas library. By reading the geojson file, we can observe that the data is structured in geojson format, consisting of two columns: geometry and class. The geometry column contains the coordinates of the points, while the class column denotes the corresponding class for each point. Notably, we observe that there are two classes: Burned and Unburned.\n\ntraining_path = './data/BA_training_pixels_2.geojson'\ntraining_pixels = gpd.read_file(training_path)\ntraining_pixels.head()\n\n\n\n\n\n\n\n\nclass\nland_class\ngeometry\n\n\n\n\n0\n13.0\nBurned\nPOINT (739290.000 2659620.000)\n\n\n1\n1.0\nUnburned\nPOINT (730050.000 2650620.000)\n\n\n2\n13.0\nBurned\nPOINT (762240.000 2649120.000)\n\n\n3\n5.0\nBurned\nPOINT (798120.000 2648130.000)\n\n\n4\n4.0\nUnburned\nPOINT (781020.000 2646150.000)\n\n\n\n\n\n\n\nLet’s take a look at the distribution of the classes.\n\ntraining_pixels['land_class'].value_counts()\n\nUnburned    447\nBurned      253\nName: land_class, dtype: int64\n\n\nCreate a dictionary with class labels for later use.\n\nclasses = np.unique(training_pixels['land_class'])\nclass_dict = dict(zip(list(classes), range(len(classes))))\n\nclass_dict\n\n{'Burned': 0, 'Unburned': 1}\n\n\n\n\nRaster data\nWe read the downloaded image using rasterio and check for the projection match with the vector data. It is important to have both the training points and the raster image to same projection since we’ll overlay the points on the raster to extract the band values to build the training data.\n\n\nraster_path = './data/145044_20090425_ms_pc.tif'\nraster = rasterio.open(raster_path)\nbands = raster.read()\n\n# check if the both files are in same crs\nassert training_pixels.crs == raster.crs , \"crs are not the same\""
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#visualise-the-data",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#visualise-the-data",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Visualise the data",
    "text": "Visualise the data\nWe visualise the data by overlaying the points on top of the raster for a visual inspection. Taking a closer look at the histogram, we notice a significant number of outliers present on the edges. To address this, we apply a 2% stretching technique to the raster. By doing so, we enhance the contrast and improve the overall visual representation, allowing us to better discern the details within the image.\n\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\nfig, ax = plt.subplots(figsize = (10,10))\n\n# create the rgb bands\nrgb  = np.stack((bands[4], bands[3], bands[2]), axis =0)\n\n# Apply contrast stretching\n# get the 2, 98 percentiles for clipping\n\np2, p98 = np.nanpercentile(rgb, (2, 98))\n\n# clip the values within the range\n# every value below 2nd percentile and above 98th percentile will be truncated to the given percentile values\n\nrgb = np.clip(rgb, p2, p98)\n\n# apply the normalisation  (0-1) \nrgb = (rgb - p2) / (p98 - p2)\n\n\n# plot the image\ntraining_pixels.plot(ax =ax, color = 'red', alpha = .8, markersize = 8, marker = '+')\n\nshow(rgb, ax = ax, transform=raster.transform)\nplt.show()"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#data-prep",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#data-prep",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Data Prep",
    "text": "Data Prep\nTo create our training data, we need to gather the band values for each point in our labelled training samples and raster image. We do this by iterating through the points while traversing the raster and saving the values of each band into an array as rows.\nWe use the latitude and longitude values from the points as an index to locate the corresponding pixel position and its respective row and column position in our data arrays. Once we have this information, we extract the values for each pixel across all the bands.\nIf your labelled training samples are not points, but rather polygons or other shapes, you can rasterize them using the rasterio.rasterize() function and extract the band values for each polygon. More information on this process can be found in the rasterio documentation.\nWe also use the label values from each point to add them to the respective label dataset. This ensures that we have the appropriate labels assigned to their corresponding data points.\n\nband_vals =[] \nlabels = []\n\nfor index, row in training_pixels.iterrows():\n    x = row['geometry'].x\n    y = row['geometry'].y\n    label = row['land_class']\n    \n    # get the respective row, col\n    row, col = raster.index(x,y)\n\n    #get the data for all bands for that pixel\n    data = bands[:, row, col]\n\n    # add it to the X\n    band_vals.append(data)\n    \n    # add the respective label class to y\n    labels.append(class_dict[label])\n\n\n# convert band_vals and labels to full array \n\nX = np.array(band_vals)\n\ny = np.array(labels)\n\n# check if they have same length\n(X.shape, y.shape)\n\n((700, 6), (700,))\n\n\nWe can observe that the our new data has 700 rows and 6 cols. These 6 cols are the bands values for each point Since we have some training data outside the image, we should remove. This is important since the algorithm won’t run with NaN values.\n\n\n\n# mask values for the rows with nan\nnan_mask = np.any(np.isnan(X), axis =1)\n\n# remove the rows with the mask =TRUE\nX = X[~nan_mask]\n\n# apply the same for y\ny= y[~nan_mask]\n\n(X.shape, y.shape)\n\n((685, 6), (685,))"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#indices",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#indices",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Indices",
    "text": "Indices\nIndices are mathematical combinations of bands that are used to highlight a specific feature in the image.\nOne might ask, why do we need indices if already have those bands in the dataset. Answer is that, indices provide more information and highlight a specific characteristic of a pixel like vegetation,water etc. Thus, providing the algorithm with more patterns leading to improved results.\nWe add following indices to our dataset:\n\n\n\n\n\n\n\n\nIndice\nDescription\nLandsat 5 Calculation\n\n\n\n\nNDVI (Normalised Difference Vegetation Index)\nMeasure of vegetation and it is calculated as the difference between NIR and Red band. This is helpful since the burned area lacks vegetation.\n(Band 4 - Band 3) / (Band 4 + Band 3)\n\n\nNDWI (Normalised Difference Water Index)\nMeasure of water and it is calculated as the difference between NIR and SWIR band. This is helpful since the burned area gets confused with water.\n(Band 5 - Band 3) / (Band 5 - Band 3)\n\n\nNDMI (Normalised Difference Moisture Index)\nMeasure of moisture and it is calculated as the difference between NIR and SWIR band. This is helpful since the burned area lacks moisture.\n(Band 5 - Band 4) / (Band 5 + Band 4)\n\n\nBAI (Burned Area Index)\nMeasure of burned area and it is calculated as the difference between NIR and SWIR band.\n1.0 / ((0.1 - Band 4)^2 + (0.06 - Band 5)^2)\n\n\nNBR (Normalised Burned Ratio)\nMeasure of burned area and it is calculated as the difference between NIR and SWIR band.\n(Band 4 - Band 7) / (Band 4 + Band 7)\n\n\n\nWe write function that take the arrays and provide an ouput value for each row in an array\n\n\n# we calculate and use np.expand to add a dimension so that i match with the X which is needed when we add it to the main data\n\ndef indice_calc(array_in, band1, band2):\n    return np.expand_dims((array_in[:,band1]- array_in[:,band2])/(array_in[:,band1]+ array_in[:,band2]), axis = 1)\n\n\nndvi = indice_calc(X,3,2)\nndwi = indice_calc(X,4,2)\nndmi = indice_calc(X, 4,3)\nnbr = indice_calc(X, 3,5)\n\nbai = np.expand_dims((1.0 / ((0.1 - X[:,3]) ** 2 + (0.06 - X[:,4]) ** 2)), axis =1)\n\n# add the indices to the X\nX = np.concatenate([X, ndvi, ndwi, ndmi, nbr, bai], axis =1)\n\nOur final training data looks like this this. Every row is a point and every col is a band or an indice value. We have 11 cols and 685 rows. The first 6 cols are the band values and the last 5 cols are the indices values. We will have a corresponding label for each row in the label array.\n\ngpd.GeoDataFrame(X).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n0\n0.194597\n0.162587\n0.114710\n0.282103\n0.341915\n0.263375\n0.421843\n0.497575\n0.095851\n0.034332\n8.878047\n\n\n1\n0.183350\n0.151917\n0.103737\n0.248635\n0.325855\n0.273330\n0.411205\n0.517042\n0.134415\n-0.047312\n10.779202\n\n\n2\n0.135280\n0.120293\n0.086110\n0.173890\n0.235545\n0.210823\n0.337615\n0.464582\n0.150586\n-0.096000\n27.566603\n\n\n3\n0.177575\n0.144685\n0.101400\n0.262082\n0.305038\n0.227542\n0.442064\n0.501030\n0.075742\n0.070544\n11.585591\n\n\n4\n0.191132\n0.158243\n0.113252\n0.247947\n0.306605\n0.259580\n0.372910\n0.460519\n0.105774\n-0.022920\n12.091535"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#model-building",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#model-building",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Model building",
    "text": "Model building\nIn machine learning models we train on a subset of data and test on the remaining data. We use the train_test_split function from sklearn to split the data into train and test sets. We use 80% of the data for training and 20% for testing. We also use stratify option to make sure that the class distribution is same in both train and test sets.\n\nfrom sklearn.model_selection import train_test_split\n\n# we split 80:20 for training and test\n# we use stratified split based on the class for balances train and test data\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .2, random_state =42, stratify = y)\n\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((548, 11), (137, 11), (548,), (137,))\n\n\nSince we don’t have equal number of samples for each class we create a dictionary that specifies the relative class distribution for our data. This would helps the algorithm to reduce the bias. We obtain the class weights by calculating the reciprocal of the class counts. These weights will assign higher importance to minority classes during model training, helping to mitigate the effects of class imbalance.\n\nlabels, counts = np.unique(y_train, return_counts = True)\nclass_weight_dict = dict(zip(labels, 1/counts))\nclass_weight_dict\n\n{0: 0.005, 1: 0.0028735632183908046}\n\n\n\nTrain the model\nRandom Forest is a supervised classification algorithm that uses ensemble learning method for classification. Ensemble learning is a type of learning where you join different types of algorithms or same algorithm multiple times to form a more powerful prediction model. The random forest algorithm combines multiple algorithm of the same type i.e. multiple decision trees, resulting in a forest of trees, hence the name “Random Forest”. In general, the more trees in the forest the more robust the forest looks like. In the same way in the random forest classifier, the higher the number of trees in the forest gives the high accuracy results.\nIt uses decision trees to classify the data into pre-defined classed and then utlise the majority voting to decide the final class. Some of the parameters are:\n\nNumber of Trees : Number of trees in the forest.\nNumber of Varibales per Split: Number of variables to consider when looking for the best way to split the data at each node of a decision tree.\nClass Weight: Weights associated with classes. If not given, all classes are supposed to have weight one.\n\n\n# build the model\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(\n    # number of decision trees\n    n_estimators=200,\n    # class weights\n    class_weight= class_weight_dict,\n    # n splits per tree\n    max_depth=2,\n    # parallelisation needed or not\n    n_jobs = 1,\n    # progress indicator\n    verbose =1,\n    random_state = 42\n)\n\n\n\nFit the model\n\n# fit the model\nclf.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.1s finished\n\n\nRandomForestClassifier(class_weight={0: 0.005, 1: 0.0028735632183908046},\n                       max_depth=2, n_estimators=200, n_jobs=1, random_state=42,\n                       verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(class_weight={0: 0.005, 1: 0.0028735632183908046},\n                       max_depth=2, n_estimators=200, n_jobs=1, random_state=42,\n                       verbose=1)\n\n\n\n\nPredict\nWe predict on test data using predict function and then calculate the accuracy score using accuracy_score function.\n\npreds = clf.predict(X_test)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n\n\n\n\nAccuracy Assement\nWe use the confusion matrix to assess the accuracy of the model. The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.\nThere is slight difference in the terms used for accuracy assessment in both remote sensing and machine learning. In remote sensing, we use the terms producer’s and user’s accuracy. In machine learning, we use the terms precision and recall. They are defined as follows:\n\nOverall accuracy: The proportion of correctly classified samples out of the total number of samples in the dataset.\nUser’s accuracy / Precision: The proportion of correctly classified samples for a given class out of the total number of samples predicted to be in that class.\nProducer’s accuracy / Recall: The proportion of correctly classified samples for a given class out of the total number of samples that are actually in that class.\n\nWe will also use F1 score to assess the accuracy of the model. F1 score is the harmonic mean of precision and recall. It is a good measure of model accuracy for imbalanced datasets. It is calculated as follows:\nF1 = 2 * (precision * recall) / (precision + recall)\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, preds)\n# normalised cm\ncm = cm.astype('float')/cm.sum(axis =1)[:, np.newaxis]\n\n# Plot confusion matrix as heatmap with class labels\nax = sns.heatmap(cm, annot=True, cmap='Blues', \n            xticklabels=class_dict.keys(), \n            yticklabels=class_dict.keys())\n\nax.set_title('Noramalised Confusion Matrix')\nax.set_xlabel(\"Predicted Label\")\nax.set_ylabel('True Label')\n\n\nsns.set()\n\n\n\n\n\n\n\n\n\n# calculate the overall accuracy\noverall_accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n\nprint(f'The overall accuracy is {overall_accuracy:.2f}.')\n\nThe overall accuracy is 0.81.\n\n\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Calculate precision for the burned class\nclass_precision = precision_score(y_test, preds, labels=[0], average=None)\nprint(f\"User's accuracy/Precision for Burned Class : {class_precision[0]:.2f}\")\n\n# Calculate recall for the burned class\nclass_recall = recall_score(y_test, preds, labels=[0], average=None)\nprint(f\"Producer's accuracy/Recall for Burned class: {class_recall[0]:.2f}\")\n\n# Calculate F1 score for the burned class\nclass_f1_score = f1_score(y_test, preds, labels=[0], average=None)\nprint(f\"F1 score for Burned class: {class_f1_score[0]:.2f}\")\n\nUser's accuracy/Precision for Burned Class : 0.68\nProducer's accuracy/Recall for Burned class: 0.84\nF1 score for Burned class: 0.75\n\n\nAs we can see from the confusion matrix, the model is able to predict the burned class with 84% accuracy. The model is able to predict the non-burned class with 77% accuracy. The overall accuracy of the model is 81%. But it is important to note that the overall accuracy is not a good measure of model accuracy for imbalanced datasets. Hence, we use F1 score to assess the accuracy of the model."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#hyper-parameter-tuning",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#hyper-parameter-tuning",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Hyper parameter tuning",
    "text": "Hyper parameter tuning\nBefore we finalise our model, we need to find the best parameters for the model. This is a process of finding the best parameters for the model that gives the best accuracy is called hyper parameter tuning. We use GridSearchCV function from sklearn to find the best parameters for our model. We provide a range of values for each parameter and the GridSearchCV function will try all the combinations of the parameters and find the best parameters for our model. We use the best parameters to train our model and then use that to predict all over the image.\nGridSearchCV also allows you to cross-validate your parameters. Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. In k-fold cross-validation, you split the input data into k subsets of data (also known as folds). You then iterate over k ML models. In each iteration, you use one of the k subsets as the test set (also called the validation data) and the union of the other subsets as the training set. For example, suppose you have 5000 input samples. You split the data into 5 folds of 1000 samples each. You train the ML model on 4 of the 5 folds (4000 samples) and evaluate it on the remaining 1 fold (1000 samples). You repeat this process 5 times (each time with a different fold as the evaluation dataset) and average the accuracy scores obtained in all the 5 iterations to get a final score for the ML model.\nIn our case, we use the same parameters as before and provide a range of values for each parameter. We use 5-fold cross-validation to find the best parameters for our model. We use the best parameters to train our model and then use that to predict all over the image.\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, make_scorer\n\n# create a classifier object\n\nclf = RandomForestClassifier(random_state=10, class_weight=class_weight_dict)\n\n# the parameter grid to be tested\n\nparam_grid = {\n    'n_estimators':list(range(100,500,100)),\n    'max_depth': [3,5,7],\n    # minimum numbee required to split another internal node\n    'min_samples_split': [2,4,6]\n}\n\n# perform grid-search with cross validation\n# we choose f1 score to as the metric to compare score\n\ngrid_search = GridSearchCV(clf, param_grid,cv = 5, scoring='f1')\ngrid_search.fit(X_train, y_train)\n\n# extract the best parameters and best score\n\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Parameters:\", best_params)\nprint(\"Best F1 Score:\", best_score)\n\n# we create a new clf with best params\nfinal_clf = RandomForestClassifier(random_state=10, class_weight=class_weight_dict, **best_params)\n\n# fit the model with the best params\nfinal_clf.fit(X_train, y_train)\n\nBest Parameters: {'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200}\nBest F1 Score: 0.8899791686173177\n\n\nWe can see that the F1 score has improved from 0.86 to 0.87. We can also see that the best parameters for the model are max_depth = 7, min_samples_split = 6 and n_estimators = 100. We use these parameters to train our model and then use that to predict all over the image."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#predict-over-the-entire-image",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#predict-over-the-entire-image",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Predict over the entire image",
    "text": "Predict over the entire image\nWe have built our model based on the training data from the full image which is a small portion. Now we will use the model to predict the burned area for the entire image. We will use the predict function to predict the burned area for the entire image. We are going by small patches to minimise memory use and to avoid crashes. For, that we read our full image and the collect the profile data to write the metadata for the classified band. We read the bands later by specifying a small patch of the image using window function. Parallel to this we open the image to saved in the write mode and then write onto it window by window.\n\nimport datetime\n\nstart_time = datetime.datetime.now()\n\n# reading raster windows\n# input raster\nl5_image  = raster_path\n\n# output file\noutput_image = './data/l5_ms_pc_classified.tif'\n\n# open the full image using rasterio\n\n\n\nwith rasterio.open(l5_image, 'r') as src:\n    profile = src.profile\n    #update profile for the classification ouput\n    profile.update(\n        # since we have nan\n        dtype = rasterio.float32,\n        # specify the band n\n        count = 1\n    )\n    # open the output file in write mode\n    with rasterio.open(output_image, 'w', **profile) as dst:\n\n        # specify the patch size\n        patch_size = 1000\n        \n        rows = src.shape[0]//patch_size+1\n        cols = src.shape[1]//patch_size+1\n        #rows = 2\n        #cols =2 \n        for i in range(rows):\n            for j in range(cols):\n                window = rasterio.windows.Window(\n                    row_off = i * patch_size,\n                    col_off = j * patch_size,\n                    # adding to ensure this doesn't go out of the image\n                    height = min(patch_size, src.shape[0] - i* patch_size),\n                    width = min(patch_size, src.shape[1] - j*patch_size)\n                )\n                #print(f\"Current window is {window.row_off, window.col_off}\")\n\n\n                # read the window\n                data = src.read(window=window)\n                \n\n                # we swap the axis since it get's the masking right\n                img_swp = np.moveaxis(data, 0,-1)\n                # now we flatten the data to 2-dim array for training\n                # -1 will infer the size of first dimension based on second dim, then band size would be used as a the second dim\n                img_flat = img_swp.reshape(-1, img_swp.shape[-1]) \n                # let's use indice_cal function to calculate indices\n\n                ndvi = indice_calc(img_flat,3,2)\n                ndwi = indice_calc(img_flat,4,2)\n                ndmi = indice_calc(img_flat, 4,3)\n                nbr = indice_calc(img_flat, 3,5)\n                bai = np.expand_dims((1.0 / ((0.1 - img_flat[:,3]) ** 2 + (0.06 - img_flat[:,4]) ** 2)), axis =1)\n                # add it to flat data using concat\n                img_w_ind = np.concatenate([img_flat,ndvi, ndwi,ndmi, nbr, bai], axis = 1)\n\n                # let's mask the invalid data before training\n\n                mask= np.ma.masked_invalid(img_w_ind)\n\n                # we apply to the data and extract only valid values\n\n                img_masked = img_w_ind[~mask.mask]\n\n                # since it converts the 2d to 1d, we need convert it back again\n\n                to_predict = img_masked.reshape(-1, img_w_ind.shape[-1] )\n\n                # some inputs will be empty especially if you have masked large portions\n                # we add a contingency option to skip that is the case\n\n                if not len(to_predict):\n                    continue\n\n                # predict\n                # returns a 1dim array with the predicted classes\n                img_pred = final_clf.predict(to_predict)\n\n                # add the data back to the valid pixels using first bands's mask as an index\n                # initiate an empty array to store the modified values after masking\n\n                output = np.zeros(img_flat.shape[0])\n\n                output[~mask.mask[:,0]] = img_pred\n\n                # convert to original image dimensions\n                # keep in mind that is only 1 band. so we need to give the row,col as an input\n\n                output = output.reshape(*img_swp.shape[:-1])\n\n                # create a final mask for writing\n                # so we convert the current mask to image dimensions but only one band\n                mask_w = (~mask.mask[:,0].reshape(*img_swp.shape[:-1]))\n\n                #write final files\n\n            \n\n                dst.write(output.astype(rasterio.uint8),1, window = window)\n\n                dst.write_mask(mask_w, window = window)\n\nend_time= datetime.datetime.now()\nprint(f\"Process finished. Time taken: {end_time-start_time}\")\n\n        \n\n\nProcess finished. Time taken: 0:03:58.447337\n\n\n\nVisualise the results\n\n# visualise the image\n\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nfig,(ax1, ax2) = plt.subplots(ncols =2, nrows= 1,figsize = (10,4), sharey=True)\n\n\n# read the classified image\nwith rasterio.open('./data/l5_ms_pc_classified.tif', 'r') as output_raster:\n    #show(output_raster)\n    classes = output_raster.read()\n\nshow(rgb, transform=raster.transform, ax = ax1)\nshow(classes, transform= output_raster.transform, ax = ax2)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that all the red areas are fire instances. Let’s quantify the percentage of area burned in the image.\n\n\nQuantify burned area\n\n# we use the classes variable to compute\n\n# count the number of pixels with 0 value excluding NaN\nclass_0_count = np.count_nonzero(classes == 0)\nclass_0_count\n\n# total pixels\ntotal_pixels = np.count_nonzero(~np.isnan(classes))\ntotal_pixels\n\nclass_0_pc = (class_0_count/total_pixels)*100\n\nprint(f\"total area of burned area in this image is: {class_0_pc:.2f}%\")\n\n# calculate in km2\nwidth_km = 170\nheight_km = 183\nl5_area_km2 = width_km * height_km\nl5_burned_area = l5_area_km2*(class_0_pc/100)\n\nprint(f\"total area of burned area in this image is: {l5_burned_area:.2f} KMsq\")\n\n\ntotal area of burned area in this image is: 52.74%\ntotal area of burned area in this image is: 16407.51 KMsq\n\n\nWe can observe that half the image area is burned and that is around ~ 16000 Kilometer Square Area."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#conclusion",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#conclusion",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Conclusion",
    "text": "Conclusion\nWe have successfully built a model and use it to predict over the image. This shows that we can use machine learning to predict fire over large areas. We can use this model to predict fire over large areas and then use that information to take preventive measures.\nThere are many ways to improve the model. We can use more samples for burned class to improve the accuracy of the model. We can also use other algorithms like Support Vector Machine to improve the accuracy of the model. We can also use other informaton like slope, aspect, elevation, etc. to improve the accuracy of the model. I have limited the scope of this tutorial to Random Forest Classifier with limited data. But, you can try other algorithms and see if you can improve the accuracy of the model.\nYou can find the full notbeook and the data in this github repository: Link\nNote: Feel free to ask any questions if you need further clarification or have doubts!"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Evaluating methods to map burned area at 30-meter resolution in forests and agricultural areas of Central India [2022]\nChandel, A., Sarwat, W., Najah, A., Dhanagare, S., & Agarwala, M. (2022, December 20). Evaluating methods to map burned area at 30-meter resolution in forests and agricultural areas of Central India. Frontiers in Forests and Global Change, 5. https://doi.org/10.3389/ffgc.2022.933807\nSabarimala, corruption, local body results: Why Kerala election is a tough call [2021]\nPublisher: India Today\nCo-Author: Rahul Verma \nMost BJP supporters want to take vaccines, Congress supporters not far behind [2021]\nPublisher: Hindustan Times - Live Mint\nCo-Authors: Rahul Verma and Ankita Barthwal\n\nMadhya Pradesh by-polls 2020: Advantage BJP over Congress [2020]\nPublisher: CNBC - TV18\n\nControlling India’s Coffee Market  [2018]\nPublication: Independent\nA business story - analysing how a coffee growing business turned into India’s largest coffee retailer -through the data driven journalism.\nCollaborators: Abhishek Mishra, Harish K Chandran\nIndia Doubling Petrol Pumps, Retailers Discontent [2018]\nPublication: Independent\nA data-driven story in the context of the Government's announcement to double the petrol pumps in India\nCollaborators: Abhishek Mishra, Harish K Chandran"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Biomass Estimation (DL)\nResNet/EfficientNet-based AGB predictions for India.\n\n\n   Soil Organic Carbon Prediction\nML-based SOC mapping using EU + Indian data layers.\n\n\n\n\n\n\n\n  Urban Surface Sealing\nDeveloped impervious-surface detection models for Correctiv and NDR using Sentinel-2 imagery and machine learning.\n\n🔗 Correctiv\n🔗 NDR\n\n\n\n   Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python\nThis post provides a comprehensive guide to predicting burned areas on Landsat 5 images using machine learning in Python."
  },
  {
    "objectID": "portfolio.html#environmental-ai",
    "href": "portfolio.html#environmental-ai",
    "title": "Portfolio",
    "section": "",
    "text": "Biomass Estimation (DL)\nResNet/EfficientNet-based AGB predictions for India.\n\n\n   Soil Organic Carbon Prediction\nML-based SOC mapping using EU + Indian data layers.\n\n\n\n\n\n\n\n  Urban Surface Sealing\nDeveloped impervious-surface detection models for Correctiv and NDR using Sentinel-2 imagery and machine learning.\n\n🔗 Correctiv\n🔗 NDR\n\n\n\n   Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python\nThis post provides a comprehensive guide to predicting burned areas on Landsat 5 images using machine learning in Python."
  },
  {
    "objectID": "portfolio.html#conflict-crisis-damage-mapping",
    "href": "portfolio.html#conflict-crisis-damage-mapping",
    "title": "Portfolio",
    "section": "Conflict & Crisis Damage Mapping",
    "text": "Conflict & Crisis Damage Mapping\n\n\n\n\nUkraine Conflict Damage Mapping\nContributed SAR-based proxy damage maps and visual analysis of destruction in Ukrainian cities including Mariupol.\n\n🔗 Tagesspiegel – Ukraine Invasion Overview\n🔗 Süddeutsche Zeitung – Mariupol Destruction\n🔗 Urban Journalism – Ukraine Destruction\n🔗 iStories\n\n\n\n\nGaza Conflict Damage Mapping\nMapped building-level destruction, verified conflict damage, and supported media analysis with satellite data.\n\n🔗 Stern – Systematic Destruction in Gaza\n🔗 Süddeutsche Zeitung – Visual Destruction Summary\n🔗 Handelsblatt – Satellite Imagery from Gaza Conflict\n🔗 NZZ – Gaza Anniversary Retrospective"
  },
  {
    "objectID": "portfolio.html#geospatial-ml-tools",
    "href": "portfolio.html#geospatial-ml-tools",
    "title": "Portfolio",
    "section": "🌍 Geospatial ML & Tools",
    "text": "🌍 Geospatial ML & Tools\n\n\n\n  \nGaza Damage Map Portal\nDeveloped an interactive website for visualising Gaza building damage; full-stack delivery from layers to interface.\n\n\n   Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine\n…estimate the extent of urbanization in Delhi using ML + remote sensing…\n\n\n   Turkey Earthquake Damage Mapping\nMapped estimated building damage using remote sensing (NZZ).\n\n\n\n\n\n\n   Rhine Flood Dashboard\nGEE-powered tool to visualize flooding and extent change.\n\n\n   Compare Spectra App: A Simple and User-Friendly Tool for Spectral Analysis\nThe compare-spectra app simplifies spectral analysis for remote sensing projects."
  },
  {
    "objectID": "portfolio.html#data-visuals-public-impact",
    "href": "portfolio.html#data-visuals-public-impact",
    "title": "Portfolio",
    "section": "🧭 Data, Visuals & Public Impact",
    "text": "🧭 Data, Visuals & Public Impact\n\n\n\n   Data Science Slides\nMy teaching slides from the data science class.\n\n\n   Data Viz gallery\nA few of my data visualisations\n\n\n   Data Science works at CPR"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please use the form below to send me a message.\nI’ll try to get back to you as soon as possible.\n\n\nName \nEmail \nMessage\n\n\n\nSend"
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "Please use the form below to send me a message.\nI’ll try to get back to you as soon as possible.\n\n\nName \nEmail \nMessage\n\n\n\nSend"
  },
  {
    "objectID": "portfolio2.html",
    "href": "portfolio2.html",
    "title": "Portfolio",
    "section": "",
    "text": "Above Ground Biomass Estimation with Deep Learniing\n\n\n\n\n\n\nNajah\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nSoil Organic Carbon Estimation with Machine Learning\n\n\n\n\n\n\nNajah\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide\n\n\n\nRemote Sensing\n\nMachine Learning\n\nPython\n\n\n\nThis post provides a comprehensive guide to predicting burned areas on Landsat 5 images using machine learning in Python. It covers all the steps involved in the process…\n\n\n\nNajah\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine\n\n\n\nGEE\n\nRemote Sensing\n\nMachine Learning\n\njavascript\n\n\n\nIn this blog post, we explore the use of machine learning techniques and Sentinel-2 satellite imagery in Google Earth Engine to estimate the extent of urbanization in Delhi.…\n\n\n\nNajah\n\n\nJun 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare Spectra App: A Simple and User-Friendly Tool for Spectral Analysis\n\n\n\nnews\n\ncode\n\napp\n\njavascript\n\n\n\nThe compare-spectra app is a new Earth Engine app that simplifies spectral analysis for remote sensing projects. It’s a easy a tool that makes it easy to explore and compare…\n\n\n\nNajah\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Viz gallery\n\n\n\nR\n\ncode\n\nViz\n\n\n\nA few of my data visualisations\n\n\n\nNajah\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\nSpatial Viz gallery\n\n\n\nViz\n\n\n\nA few of of my vector and raster maps\n\n\n\nNajah\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Slides\n\n\n\nR\n\ncode\n\nteaching\n\n\n\nMy teaching slides from the data science class\n\n\n\nNajah\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science works at CPR\n\n\n\nR\n\ncode\n\n\n\n\n\n\n\nNajah\n\n\nJan 31, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Najah",
    "section": "",
    "text": "👋 Hi, I’m Najah\nI’m a data scientist working at the intersection of satellite imagery, remote sensing, and machine learning. I’ve led projects spanning climate, conflict, conservation, and urban change—leveraging geospatial intelligence for impact.\nMost recently, I’ve worked with two Earth observation startups: one on monitoring natural capital and the other on delivering satellite insights to newsrooms. Projects include:\n\n🌱 Biomass estimation using multi-sensor data\n\n🏚️ Damage mapping in Gaza & Ukraine with SAR interferometry\n\n🏕️ Soil organic carbon prediction models for India\n🌆 Impervious area detection in German cities\n\nI’ve presented at global venues like COP28 and Google Geo for Good.\nI also teach data science at Ashoka Centre and am now looking for part-time, full-time, or contract roles in applied geospatial AI.\n\n\n\n\n\n\n\n\n🗣️ “I’m passionate about using data science for environmental and social impact.”\n\n\n📍 Based in Berlin | 🌏 Working on global problems\n\nView My Portfolio Download CV"
  },
  {
    "objectID": "posts/biomass-dl/index.html",
    "href": "posts/biomass-dl/index.html",
    "title": "Above Ground Biomass Estimation with Deep Learniing",
    "section": "",
    "text": "I’ve been working on the problem of biomass estimation in India. It’s a complex challenge for a few reasons:\n\nLack of high-quality, high-resolution training data\nDifficulty in mapping without LiDAR support\nComplex, nonlinear relationships between biomass and input features\nSaturation of biomass estimates beyond 350 Mg/ha\n\nWe initially worked with NASA’s GEDI data. However, it consistently underestimated values for the specific geographies I was working on. The resolution and forest structure in India likely didn’t align well with GEDI’s assumptions.\nEventually, we found a more reliable dataset: above-ground biomass (AGB) maps generated by ISRO using airborne LiDAR and field plots. This dataset was of significantly higher quality, and we decided to move forward with it. We had five field sites across India to work with.\nInput Data and Early Models\nThere were many combinations to try—different sensors, resolutions, temporal layers, and derived features. I’ll cover those explorations in detail in a future post.\nWe focused primarily on:\n\nOptical imagery: Sentinel-2, Landsat 8\nRadar data: Sentinel-1 (dual-pol), PALSAR\nTemporal layers: Three seasonal composites (pre-monsoon, monsoon, post-monsoon)\nDimensionality reduction: PCA, yielding 99 input features\n\nThe first approach used basic machine learning models (Random Forest, XGBoost) on tabular features extracted from the raster stack. The accuracy plateaued around 60–70% (R²), with RMSE often exceeding 50 Mg/ha.\nPatchwise Deep Learning\nTo introduce spatial awareness, I moved to patch-based modeling. This worked better. I trained models like ResNet and Fully Convolutional Networks (FCNs) on 16×16 pixel patches. Performance improved significantly—R² reached above 85%, and RMSE dropped to around 35 Mg/ha. But the outputs were still patch-level, and sliding-window inference blurred the spatial detail.\nTowards Pixelwise Predictions\nThat led me to redesign the workflow for pixelwise modeling. After many iterations and simplifications, I settled on a ResNet-style convolutional regression model that predicts directly at the pixel level.\nTo add spatial context (and avoid the pitfalls of purely pixel-based CNNs), I introduced several enhancements:\n\nSurrounding context as multi-channel inputs\nMulti-season temporal composites\nSpatially smoothed biomass targets to aid learning\nExperimental use of attention bottlenecks (still under testing)\n\nWith this pixelwise approach, we now reach R² ≈ 0.9, RMSE &lt; 30 Mg/ha, and MAE &lt; 20 Mg/ha—which is on par or better than what I’ve seen in several peer-reviewed studies.\nModel Access & What’s Next\nI’m still finalizing documentation, but the code and models are now public:\n\nTraining code: https://github.com/vertify-earth/biomass-dl-model-training\nInference pipeline: https://github.com/vertify-earth/biomass-dl-model-training/blob/main/biomass_inference_notebook_Version2.ipynb\nModel hub: https://huggingface.co/vertify/biomass-model\nLive demo: https://vertify-biomass-prediction-app.hf.space/\n\nI’ll write a more detailed post later covering all the experiments, failed directions, and lessons learned.\nThanks for reading."
  },
  {
    "objectID": "posts/d3s/index.html",
    "href": "posts/d3s/index.html",
    "title": "Data Science Slides",
    "section": "",
    "text": "During the spring of 2023, I had the incredible opportunity to contribute as a teaching fellow for an undergraduate course at Ashoka University. The course, titled “Data Science for Social Science,” was primarily aimed at students studying Political Science and Economics. As part of the teaching team, my responsibilities included leading weekly discussion sections, where I provided in-depth reviews of the data science methods covered in the main lectures.\nIn these discussion sections, my approach involved a combination of practical examples, live coding demonstrations, and addressing students’ queries and challenges with their coding tasks. Each week, I conducted six discussion sections, with an average of 15 students attending each session. Overall, the class consisted of approximately 90 students.\nTo showcase my contributions and provide insight into my teaching approach, I have compiled a selection of slides from my discussion sections. These slides capture the essence of the topics covered and the interactive learning environment fostered during the course.\nFeel free to explore the following slides, which offer a glimpse into my instructional techniques and the impact I had on the students’ understanding of data science in the context of social sciences.\nIntroduction to R and R Studio\nData Manipulation with dplyr\nMerging Data\nData Visualisation using ggplot2\nPredicting Housing Prices using Linear Regression\nPython Basics"
  },
  {
    "objectID": "posts/cpr-pol-connect/index.html",
    "href": "posts/cpr-pol-connect/index.html",
    "title": "Data Science works at CPR",
    "section": "",
    "text": "In this blog you will find some of the data science work that I was involved in as an analyst at the Centre for Policy Research, New Delhi\nVisit the showcase website here"
  },
  {
    "objectID": "posts/gee-urbanisation-ml-sentinel-2/index.html",
    "href": "posts/gee-urbanisation-ml-sentinel-2/index.html",
    "title": "Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine",
    "section": "",
    "text": "In this post, I am sharing a project I completed as part of my machine learning course at Ashoka University. The objective was to leverage machine learning to understand the extent of urbanisation in the city of Delhi. Using the concept of built-up areas as a proxy for urban areas, I applied Random Forest machine learning algorithm to estimate the built-up area in Delhi."
  },
  {
    "objectID": "posts/gee-urbanisation-ml-sentinel-2/index.html#introduction",
    "href": "posts/gee-urbanisation-ml-sentinel-2/index.html#introduction",
    "title": "Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine",
    "section": "",
    "text": "In this post, I am sharing a project I completed as part of my machine learning course at Ashoka University. The objective was to leverage machine learning to understand the extent of urbanisation in the city of Delhi. Using the concept of built-up areas as a proxy for urban areas, I applied Random Forest machine learning algorithm to estimate the built-up area in Delhi."
  },
  {
    "objectID": "posts/gee-urbanisation-ml-sentinel-2/index.html#methodology",
    "href": "posts/gee-urbanisation-ml-sentinel-2/index.html#methodology",
    "title": "Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine",
    "section": "Methodology:",
    "text": "Methodology:\n\nObtaining Delhi Boundary Geometry:\nTo initiate the project, I obtained the precise boundary geometry of Delhi using the FAO/GAUL/2015/level2 dataset. This enabled me to focus specifically on the area of interest and streamline the analysis.\n\nvar delhi = ee.FeatureCollection(\"FAO/GAUL/2015/level2\").filter(ee.Filter.eq('ADM2_NAME','Delhi')).geometry();\n\n\n\n\nFiltering Sentinel-2 Imagery:\nNext, I accessed the Sentinel-2 satellite imagery from the COPERNICUS/S2_SR image collection. To ensure high-quality data, I filtered the imagery based on a cloud cover percentage of less than 30% and a date range of January 1, 2019, to December 31, 2019. Additionally, I restricted the imagery to be within the bounds of Delhi, using the previously acquired boundary geometry. Subsequently, I selected the relevant spectral bands (B4, B3, B2) for the analysis.\n// Code snippet 1: Filtering Sentinel-2 imagery\nvar sentinel2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n  .filterBounds(delhi)\n  .filterDate(\"2019-01-01\", \"2019-12-31\")\n  .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 30))\n  .select([\"B4\", \"B3\", \"B2\"]);\n\n\nCreating the Composite Image:\nAfterwards, I created a composite image by taking the median of the filtered images and clipped it to the boundary of Delhi. The resulting composite image provided a comprehensive representation of the land cover in the region.\nI then created a composite image by taking the median of the filtered images and clipped it to the boundary of Delhi. This composite image provided a comprehensive representation of the land cover in the region.\n// Code snippet 2: Creating the composite image\nvar composite = sentinel2.median().clip(delhi);\n\n\nVisualising the Composite Image\nTo visualise the composite image, I applied a colour visualisation scheme using the red, green, and blue bands. This enhanced the image and provided valuable insights into the distribution of different land cover types within Delhi.\n\n// Code Snippet: Visualising the Composite Image\n\n// Apply colour visualization parameters\nvar visualizationParams = {\n  bands: ['B4', 'B3', 'B2'],\n  min: 0.0,\n  max: 3000\n};\n\n// Display the composite image\n\nMap.addLayer(compositeImage, visualizationParams, 'Composite Image');\nComposite Image\n\n\n\n\n\n\n\nComposite image of Delhi\n\n\n\n\nLand Cover Classification Training Data:\nI manually annotated approximately 500 urban and non-urban points across Delhi to create the training data. I uploaded them to GEE assets as feature collection. I utilised that feature collection consisting of urban and non-urban points. Merging these collections provided a comprehensive set of training data for the classification process.\n// Code Snippet: Utilising the Feature Collection for Classification\n// Load the feature collections of urban and non-urban points\nvar urbanPoints = ee.FeatureCollection(\"projects/ee-najah/assets/urban_points\");\nvar nonUrbanPoints = ee.FeatureCollection(\"projects/ee-najah/assets/non_urban_points\");\n\n// Merge the urban and non-urban points collections\nvar trainingData = urbanPoints.merge(nonUrbanPoints);\n\n\nOverlaying Training Points and Extracting Training Data:\nNext, I overlayed the training points on the composite image to extract the necessary training data. This involved sampling regions within a specified scale of 10 units using the “sampleRegions” function, which resulted in the training data consisting of land cover labels associated with each region.\n// Code Snippet: Overlaying Training Points on the Composite Image\n// Sample regions within a specified scale of 10 units\nvar trainingData = compositeImage.sampleRegions({\n  collection: trainingData,\n  scale: 10,\n  properties: ['land_cover'],\n});\n\n\nSplitting the Dataset:\nTo evaluate the accuracy of the classification model, I split the dataset into training and testing sets. The training set was used to train the machine learning model, while the testing set was utilized to assess the model’s performance.\n\n\n// Code snippet 5: Splitting the dataset into training and testing sets\nvar split = 0.8; // 80% for training, 20% for testing\n\nvar trainingData = trainingData.randomColumn('split')\nvar training = trainingData.filter(ee.Filter.lt('split', split));\nvar testing = trainingData.filter(ee.Filter.gte('split', split));\n\n\nTraining the Random Forest Classifier:\nUsing the training data, I trained a random forest classifier with the “smileRandomForest” algorithm provided by Earth Engine. The classifier was trained with 50 trees, and the land cover property was used as the target class. The input properties for classification were derived from the band names of the composite image.\n// Code Snippet: Training the Random Forest Classifier\n\n// Train a random forest classifier with 500 trees\nvar classifier = ee.Classifier.smileRandomForest(500)\n  .train({\n    features: training,\n    classProperty: 'land_cover',\n    inputProperties: ['B4', 'B3', 'B2']\n  });\n\n\nApplying the Classifier to Generate Land Cover Classification Map\nFinally, I applied the trained classifier to the composite image to generate a land cover classification map. The classified image assigned distinct colours to different land cover classes, with the colour palette including shades of grey, brown, blue, and green. This visualisation provided a clear understanding of the land cover distribution in Delhi during the year 2019.\n// Code Snippet: Applying the Classifier to Generate Land Cover Classification Map\n\n// Apply the trained classifier to the composite image\nvar classified = compositeImage.classify(classifier);\n\n// Display the land cover classification map\nMap.addLayer(classified, { palette: ['gray', 'red'] }, 'Land Cover Classification');\nLand Classification Map\n\n\n\n\n\n\n\nImage: Land cover classification map of Delhi\n\n\n\n\nAccuracy Assessment:\nTo assess the accuracy of the classification model, I calculated the confusion matrix using the testing dataset. The confusion matrix provided insights into the model’s performance, including metrics such as overall accuracy, producer’s accuracy, and user’s accuracy.\n// Code snippet 7: Calculating the confusion matrix for accuracy assessment\nvar testAccuracy = testing\n    .classify(classifier)\n    .errorMatrix('class', 'classification');\n\nprint('Confusion Matrix:', testAccuracy);\nprint('Overall Accuracy:', testAccuracy.accuracy());\n\n\nCalculating Urban Area Percentage:\nTo understand the extent of urbanization, we calculated the area covered by the urban class within Delhi. We also calculated the total area of Delhi to determine the percentage of urban area.\n\n// New Code Snippet: Calculating urban area percentage\n\n// Calculating area of urban class\nvar urban = classified.select('classification').eq(1);\n\n// Calculate the pixel area in square kilometer for urban\nvar area_urban = urban.multiply(ee.Image.pixelArea()).divide(1000 * 1000);\n\n// Reducing the statistics for urban area in Delhi\nvar stat_urban = area_urban.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: delhi,\n  scale: 100,\n  maxPixels: 1e10\n});\n\n// Get the sum of pixel values representing urban area\nvar area_urban_sum = ee.Number(stat_urban.get('classification'));\n\n// Calculate the pixel area in square kilometer for Delhi\nvar area_delhi = ee.Image.pixelArea().clip(delhi).divide(1000 * 1000);\n\n// Reducing the statistics for Delhi\nvar stat_delhi = area_delhi.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: delhi,\n  scale: 100,\n  maxPixels: 1e10\n});\n\n// Get the sum of pixel values representing Delhi area\nvar area_delhi_sum = ee.Number(stat_delhi.get('area'));\n\n// Calculate the percentage of urban area\nvar percentage_urban_area = area_urban_sum.divide(area_delhi_sum).multiply(100);\n\n// Print the urban area in square kilometers\nprint('Urban Area (in sq.km):', area_urban_sum);\n\n// Print the total area of Delhi\nprint('Delhi Area (in sq.km):', area_delhi_sum);\n\n// Print the percentage of urban area\nprint('Percentage of urban area:', percentage_urban_area);\n\n\nExporting the Results:\nLastly, I exported the classified image and the confusion matrix results to Google Drive for further analysis and sharing with collaborators.\n\n// Code snippet 8: Exporting the classified image and confusion matrix to Google Drive\nExport.image.toDrive({\n  image: classified,\n  description: 'classified_image',\n  folder: 'GEE_Classification_Results',\n  scale: 10,\n  region: delhi\n});\n\nExport.table.toDrive({\n  collection: testAccuracy,\n  description: 'confusion_matrix',\n  folder: 'GEE_Classification_Results',\n  fileFormat: 'CSV'\n});"
  },
  {
    "objectID": "posts/gee-urbanisation-ml-sentinel-2/index.html#results-and-conclusion",
    "href": "posts/gee-urbanisation-ml-sentinel-2/index.html#results-and-conclusion",
    "title": "Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine",
    "section": "Results and Conclusion",
    "text": "Results and Conclusion\n\nThe land cover classification analysis using the basic Random Forest model achieved an overall accuracy of 87%, indicating the model’s effectiveness in accurately classifying land cover types in Delhi based on satellite imagery data.\nTo further improve the classification results, additional enhancements can be implemented, such as incorporating spectral indices like NDVI (Normalised Difference Vegetation Index) or NDBI (Normalised Difference Built-up Index) to provide additional information for distinguishing different land cover types.\nHyperparameter optimisation can be performed to fine-tune the Random Forest model, adjusting parameters such as the number of trees, tree depth, and feature subsampling, with the potential to achieve higher accuracy.\nThis project primarily focused on land cover classification in Delhi, but there is potential for extending the analysis to other Indian cities. Analysing the model’s performance in different urban environments would provide valuable insights into its generalisability and scalability.\nThe classification results suggest that approximately 78% of Delhi can be classified as urban, highlighting the significant urbanisation within the city.\nBy tracking the historical growth of urban areas in Delhi using satellite imagery, it is possible to gain a deeper understanding of urban expansion patterns over time.\n\nLink to Google Earth Engine script\nLink to original research paper"
  },
  {
    "objectID": "posts/compare-spectra/index.html",
    "href": "posts/compare-spectra/index.html",
    "title": "Compare Spectra App: A Simple and User-Friendly Tool for Spectral Analysis",
    "section": "",
    "text": "compare-spectra: a simple Earth Engine app for spectral analysis\nMade a small Earth Engine app to do quick spectral analysis. You can select areas, draw ROIs, and compare spectra with just a few clicks. No coding needed. Just open and start exploring.\nYou can also export the line charts as images or download the spectral data as CSV.\nMain features:\n\nSelect and compare multiple ROIs\nLine charts to visualise spectra\nExport charts and data\nFree and open source\nBuilt fully on GEE – no install needed\n\nBuilt this mostly because I wanted something lightweight and easy for quick comparisons, without opening Jupyter or writing scripts.\nHere’s the link: compare-spectra app\nLet me know if you find it useful or if you have suggestions.\nHere’s a quick demo:"
  }
]