[
  {
    "objectID": "posts/media-contributions/index.html",
    "href": "posts/media-contributions/index.html",
    "title": "Media Contributions",
    "section": "",
    "text": "This page showcases selected media collaborations under the banner of vertical52 and features where my satellite data and geospatial work has supported public understanding of conflict, urban planning, and disaster impact.\n\n\n\n\nCreated SAR-based proxy damage maps for Ukrainian cities.\n🔗 Tagesspiegel – Das unvorstellbare Ausmaß von zwei Jahren Ukraine-Invasion\n\n\n\nContributed visual and satellite-based analysis for assessing destruction in Mariupol.\n🔗 Süddeutsche Zeitung – Mariupol: Ukraine, Russland und der Krieg\n\n\n\nProduced damage maps and conducted visual verification for building destruction.\n🔗 Stern – Wie der Küstenstreifen systematisch zerstört wurde\n\n\n\nMaps and spatial analysis featured in retrospective media on one year since the Gaza war began.\n🔗 NZZ – One Year of Catastrophe: Three Voices from the Gaza Strip\n\n\n\nAssisted with satellite-based mapping of fire and explosion footprints during active conflict.\n🔗 Handelsblatt – Satellitenbilder zeigen den Krieg in Israel und Gaza\n\n\n\nSupported statistical and spatial visuals on the Gaza conflict.\n🔗 Süddeutsche Zeitung – Gaza: Zahlen des Kriegs\n\n\n\nGenerated estimates of damaged buildings and regions using remote sensing.\n🔗 NZZ – Turkey struggles to remove 200 million tons of quake debris\n\n\n\n\n\n\nBuilt ML models for estimating sealed surfaces; featured in multiple regional reports.\n🔗 Correctiv – Versiegelung deutscher Städte nimmt zu\n🔗 NDR – Klima in Hamburg: Temperatur hängt vom Stadtteil ab\n\n\n\nMapped reconstruction progress and flood-affected areas using remote sensing.\n🔗 Süddeutsche Zeitung – Wiederaufbau im Ahrtal\n\n\n\n\n\n\nDeveloped an interactive website for visualising Gaza building damage; full-stack delivery from layers to interface.\n🔗 Gaza Damage Map Portal\n\nSee more on my portfolio or GitHub."
  },
  {
    "objectID": "posts/media-contributions/index.html#damage-mapping-in-conflict-disaster-zones",
    "href": "posts/media-contributions/index.html#damage-mapping-in-conflict-disaster-zones",
    "title": "Media Contributions",
    "section": "",
    "text": "Created SAR-based proxy damage maps for Ukrainian cities featured in interactive media.\n🔗 Tagesspiegel – Das unvorstellbare Ausmaß von zwei Jahren Ukraine-Invasion\n\n\n\nContributed satellite analysis for visuals on destruction and urban collapse in Mariupol.\n🔗 Süddeutsche Zeitung – Mariupol: Ukraine, Russland und der Krieg\n\n\n\nGenerated high-resolution damage maps and validated building-level destruction patterns.\n🔗 Stern – Wie der Küstenstreifen systematisch zerstört wurde\n\n\n\nMapped estimated damaged areas and buildings after the 2023 earthquake.\n🔗 NZZ – Turkey struggles to remove 200 million tons of quake debris"
  },
  {
    "objectID": "posts/media-contributions/index.html#urban-and-environmental-monitoring",
    "href": "posts/media-contributions/index.html#urban-and-environmental-monitoring",
    "title": "Media Contributions",
    "section": "",
    "text": "Developed a machine learning model to estimate impervious surface coverage in urban areas.\n🔗 Correctiv – Versiegelung deutscher Städte nimmt zu\n\n\n\nAssisted in tracking reconstruction efforts and flood impact using satellite data and change detection techniques.\n🔗 Süddeutsche Zeitung – Wiederaufbau im Ahrtal"
  },
  {
    "objectID": "posts/media-contributions/index.html#platforms-and-public-tools",
    "href": "posts/media-contributions/index.html#platforms-and-public-tools",
    "title": "Media Contributions",
    "section": "",
    "text": "Developed an interactive mapping platform to visualise Gaza damage data. Built end-to-end layers, backend processing, and UI.\n🔗 Gaza Damage Portal\n\nSee more on my portfolio or GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Najah Pokkiri",
    "section": "",
    "text": "I am a data scientist with a keen interest in using satellite imagery, remote sensing, and machine learning to solve real-world problems. I graduated from Ashoka University in India with a liberal arts degree and am currently pursuing my master’s in Geodesy and Geoinformation at TU Berlin. Over the past few years, I’ve worked on geospatial data science projects in the domains of climate, conflict, conservation, and urban development.\nMost recently, I worked as a data scientist for two Earth observation startups—one focused on monitoring natural capital and the other on providing satellite intelligence to media organisations. My projects included estimating biomass using multi-sensor satellite data, mapping conflict damage in Gaza and Ukraine using SAR interferometry, and building machine learning models to predict soil carbon across Indian states. These projects have drawn on a range of geospatial techniques, including deep learning for remote sensing and time series analysis. I’ve also represented my work at international conferences such as COP28 and Google Geo for Good.\nIn addition to consulting, I currently work as a teaching assistant for an executive programme at the Ashoka Centre for Data Science, where I help mid-career professionals learn data science skills. I am open to short-term or part-time consulting roles in the geospatial data science domain, particularly those that combine Earth observation and machine learning for social or environmental impact."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Projects",
    "section": "",
    "text": "Media Contributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide\n\n\n\nRemote Sensing\n\nMachine Learning\n\nPython\n\n\n\nThis post provides a comprehensive guide to predicting burned areas on Landsat 5 images using machine learning in Python. It covers all the steps involved in the process…\n\n\n\nNajah\n\n\nJul 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine\n\n\n\nGEE\n\nRemote Sensing\n\nMachine Learning\n\njavascript\n\n\n\nIn this blog post, we explore the use of machine learning techniques and Sentinel-2 satellite imagery in Google Earth Engine to estimate the extent of urbanization in Delhi.…\n\n\n\nNajah\n\n\nJun 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare Spectra App: A Simple and User-Friendly Tool for Spectral Analysis\n\n\n\nnews\n\ncode\n\napp\n\njavascript\n\n\n\nThe compare-spectra app is a new Earth Engine app that simplifies spectral analysis for remote sensing projects. It’s a easy a tool that makes it easy to explore and compare…\n\n\n\nNajah\n\n\nJun 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Viz gallery\n\n\n\nR\n\ncode\n\nViz\n\n\n\nA few of my data visualisations\n\n\n\nNajah\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\nSpatial Viz gallery\n\n\n\nViz\n\n\n\nA few of of my vector and raster maps\n\n\n\nNajah\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Slides\n\n\n\nR\n\ncode\n\nteaching\n\n\n\nMy teaching slides from the data science class\n\n\n\nNajah\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science works at CPR\n\n\n\nR\n\ncode\n\n\n\n\n\n\n\nNajah\n\n\nJan 31, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Evaluating methods to map burned area at 30-meter resolution in forests and agricultural areas of Central India [2022]\nChandel, A., Sarwat, W., Najah, A., Dhanagare, S., & Agarwala, M. (2022, December 20). Evaluating methods to map burned area at 30-meter resolution in forests and agricultural areas of Central India. Frontiers in Forests and Global Change, 5. https://doi.org/10.3389/ffgc.2022.933807\nSabarimala, corruption, local body results: Why Kerala election is a tough call [2021]\nPublisher: India Today\nCo-Author: Rahul Verma \nMost BJP supporters want to take vaccines, Congress supporters not far behind [2021]\nPublisher: Hindustan Times - Live Mint\nCo-Authors: Rahul Verma and Ankita Barthwal\n\nMadhya Pradesh by-polls 2020: Advantage BJP over Congress [2020]\nPublisher: CNBC - TV18\n\nControlling India’s Coffee Market  [2018]\nPublication: Independent\nA business story - analysing how a coffee growing business turned into India’s largest coffee retailer -through the data driven journalism.\nCollaborators: Abhishek Mishra, Harish K Chandran\nIndia Doubling Petrol Pumps, Retailers Discontent [2018]\nPublication: Independent\nA data-driven story in the context of the Government's announcement to double the petrol pumps in India\nCollaborators: Abhishek Mishra, Harish K Chandran"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "",
    "text": "Forest and Agricultural fires are a major threat to the environment and human life. They can cause severe damage to the environment, property, and human life. In 2019, the Amazon rainforest wildfires burned more than 906,000 hectares of forest. In 2020, the Australian wildfires burned more than 18 million hectares of land. In 2021, the California wildfires burned more than 1.6 million hectares of land.\nThe ability to predict the burned areas on satellite images can help us to better understand the impact of wildfires on the environment and human life. It can also help us to better manage the wildfires and reduce their impact on the environment and human life.\n\n\n\nThe shaded area in this satellite image shows the extent of the burned area from the Parnitha fire in Greece. The fire, which broke out on July 23, 2023, has burned over 10,000 hectares of forest and caused widespread damage.\n\n\nIn this notebook, we will use machine learning to predict the burned areas on Landsat 5 images. We will use python to prepare the data, train the model, and evaluate the model.Specifically, we will use Random Forest algorithm to predict the burned area on the image using scikit-learn. We will follow the study area from my co-authored paper with my colleagues at Ashoka University on comparing methods to detect burned area in Central India. The paper is published in Frontiers in Forests and Global Change and can be accessed here.\nThe post is divided into 5 parts: 1). Image acquisition 2) Data preparation, 3) Model training, 4) Model evaluation and 5) Prediction on the image\n\n\nLandsat is a joint programme of the USGS and NASA. Landsat satellites image the entire Earth’s surface at a 30-meter resolution about once every two weeks. The Latest instalment on the series is Landst 9 which became operational in 2021. We would be using image from Landsat 5 which has been running from 1984 to 2013, thus providing us accessibly and capability to inquire about burned area historically. The USGS produces data in 3 categories for each satellite (Tier 1, Tier 2 and R). We will be using landsat 5 collection 2:level 2:tier 1. Tier 1 (T1) data meets geometric and radiometric quality requirements.\nThis dataset contains atmospherically corrected surface reflectance and land surface temperature derived from the data produced by the Landsat TM sensor. These images contain 4 visible and near-infrared (VNIR) bands and 2 short-wave infrared (SWIR) bands processed to orthorectified surface reflectance, and one thermal infrared (TIR) band processed to orthorectified surface temperature. We will use all the bands except the thermal bands since the thermal band has different resolution (100m) than others.\nThere are a bunch of ways to download the data:\n\nGoogle Earth Engine\nUSGS Explorer\nAWS\nMicrosoft Planetary Computer\n\nIn this post, we will use planetary computer’s open access data catalogue to download the data. The data is stored in cloud optimised geotiff format which makes it easy to access and process."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#introduction",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#introduction",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "",
    "text": "Forest and Agricultural fires are a major threat to the environment and human life. They can cause severe damage to the environment, property, and human life. In 2019, the Amazon rainforest wildfires burned more than 906,000 hectares of forest. In 2020, the Australian wildfires burned more than 18 million hectares of land. In 2021, the California wildfires burned more than 1.6 million hectares of land.\nThe ability to predict the burned areas on satellite images can help us to better understand the impact of wildfires on the environment and human life. It can also help us to better manage the wildfires and reduce their impact on the environment and human life.\n\n\n\nThe shaded area in this satellite image shows the extent of the burned area from the Parnitha fire in Greece. The fire, which broke out on July 23, 2023, has burned over 10,000 hectares of forest and caused widespread damage.\n\n\nIn this notebook, we will use machine learning to predict the burned areas on Landsat 5 images. We will use python to prepare the data, train the model, and evaluate the model.Specifically, we will use Random Forest algorithm to predict the burned area on the image using scikit-learn. We will follow the study area from my co-authored paper with my colleagues at Ashoka University on comparing methods to detect burned area in Central India. The paper is published in Frontiers in Forests and Global Change and can be accessed here.\nThe post is divided into 5 parts: 1). Image acquisition 2) Data preparation, 3) Model training, 4) Model evaluation and 5) Prediction on the image\n\n\nLandsat is a joint programme of the USGS and NASA. Landsat satellites image the entire Earth’s surface at a 30-meter resolution about once every two weeks. The Latest instalment on the series is Landst 9 which became operational in 2021. We would be using image from Landsat 5 which has been running from 1984 to 2013, thus providing us accessibly and capability to inquire about burned area historically. The USGS produces data in 3 categories for each satellite (Tier 1, Tier 2 and R). We will be using landsat 5 collection 2:level 2:tier 1. Tier 1 (T1) data meets geometric and radiometric quality requirements.\nThis dataset contains atmospherically corrected surface reflectance and land surface temperature derived from the data produced by the Landsat TM sensor. These images contain 4 visible and near-infrared (VNIR) bands and 2 short-wave infrared (SWIR) bands processed to orthorectified surface reflectance, and one thermal infrared (TIR) band processed to orthorectified surface temperature. We will use all the bands except the thermal bands since the thermal band has different resolution (100m) than others.\nThere are a bunch of ways to download the data:\n\nGoogle Earth Engine\nUSGS Explorer\nAWS\nMicrosoft Planetary Computer\n\nIn this post, we will use planetary computer’s open access data catalogue to download the data. The data is stored in cloud optimised geotiff format which makes it easy to access and process."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#importing-libraries",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#importing-libraries",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Importing Libraries",
    "text": "Importing Libraries\nWe start by importing the necessary libraries.\n\n#  pystac for STAC queries\nimport pystac\nimport pystac_client\n\n\n#  to access data on the Planetary Computer platform\nimport planetary_computer\n\n# for reading raster data\nimport rasterio\n\n# for reading and manipulating vector data\nimport geopandas as gpd\n\n\n# y for working with arrays\nimport numpy as np\n\n#  for machine learning algorithms\nimport sklearn"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#downlaoding-the-image-from-planetary-computer",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#downlaoding-the-image-from-planetary-computer",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Downlaoding the Image from Planetary computer",
    "text": "Downlaoding the Image from Planetary computer\n\nPlanetary Computer is Microsoft’s equivalent of Google Earth Engine (GEE), but with some key differences. Unlike GEE, which is primarily based on a JavaScript platform, Planetary Computer offers Python (CPU and GPU) and R notebooks for computation. This provides users with the flexibility to work in their preferred programming language for geospatial analysis.\nWhat sets Planetary Computer apart is its public data repository and open access API, which supports querying data using SpatioTemporal Asset Catalogs (STAC). STAC is a specification that defines a standardised format for describing geospatial data. It enables machine-readable structures for efficient data querying and downloading from the repository.\nIf you want to learn more about STAC, you can check out their official documentation at STAC.\nIn our specific case, accessing the desired image from Planetary Computer is quite straightforward. Simply specify the common access point, collection, and image ID, and you can retrieve the image for further analysis and processing. This simplified data retrieval process allows us to focus on our analysis tasks without unnecessary complexities.\n\n\n\n# specify the api access point\napi_url = 'https://planetarycomputer.microsoft.com/api/stac/v1/'\n\n# image collection\ncollection = 'landsat-c2-l2'\n\n# image ID\nimage_id = 'LT05_L2SP_145044_20100428_02_T1'\n\n# access the catalogue and get our image assets\ncatalogue = pystac_client.Client.open(api_url)\ncollection = catalogue.get_collection(collection)\nitem = collection.get_item(image_id)\nitem = planetary_computer.sign(item)\n\n# call the assets\nassets = item.assets\n\n\n# print the bands\nfor keys, asset in assets.items():\n    print(f\"{keys}: {asset.title}\")\n\nqa: Surface Temperature Quality Assessment Band\nang: Angle Coefficients File\nred: Red Band\nblue: Blue Band\ndrad: Downwelled Radiance Band\nemis: Emissivity Band\nemsd: Emissivity Standard Deviation Band\nlwir: Surface Temperature Band\ntrad: Thermal Radiance Band\nurad: Upwelled Radiance Band\natran: Atmospheric Transmittance Band\ncdist: Cloud Distance Band\ngreen: Green Band\nnir08: Near Infrared Band 0.8\nswir16: Short-wave Infrared Band 1.6\nswir22: Short-wave Infrared Band 2.2\nmtl.txt: Product Metadata File (txt)\nmtl.xml: Product Metadata File (xml)\ncloud_qa: Cloud Quality Assessment Band\nmtl.json: Product Metadata File (json)\nqa_pixel: Pixel Quality Assessment Band\nqa_radsat: Radiometric Saturation and Dropped Pixel Quality Assessment Band\natmos_opacity: Atmospheric Opacity Band\ntilejson: TileJSON with default rendering\nrendered_preview: Rendered preview\n\n\nTo conduct our analysis, we carefully choose the bands we need from the provided list. We specify these bands as a list of strings, using them to access the download link for each specific band. Leveraging the power of the rasterio library, we read in the bands from their respective links. Then, employing the versatility of numpy, we cleverly stack the bands together, resulting in a cohesive and comprehensive single image for further analysis.\n\n# list of bands we need\nbands_list = [ 'red', 'green','blue', 'nir08', 'swir16', 'swir22' ]\n\n\n# extract the urls\nband_urls = [assets[band].href for band in bands_list] \n\n# we use the urls to read in the image\nraster_pc = [rasterio.open(url) for url in band_urls]\nbands_pc = [band.read(1) for band in raster_pc]\n\n# we stack the individual bands\n# we specify as xis 0 since the the individual raster shape is (bands, row, col) and we want to add the rasters through the same axis\nl5_pc = np.stack(bands_pc, axis = 0)\n\n# check the shape of stacked raster\nl5_pc.shape\n\n(6, 6901, 7811)\n\n\n\n#check the data range\n(l5_pc.min(), l5_pc.max())\n\n(0, 65535)\n\n\nWe observe that the image has 6 bands, and its shape is 6 x 6901 x 7811. Additionally, we identify the presence of nodata values, which are currently represented by 0. To handle this, we will replace these nodata values with NaN (Not-a-Number), ensuring they do not affect our subsequent calculations.\n\nScaling\nThe data ranges from 0 to 65535, as it is stored in a 16-bit format. To convert this data to surface reflectance, which ranges from 0 to 1, we will use a scale and offset factor. Scaling the data to surface reflectance offers several benefits:\nEasier Interpretation: Surface reflectance values are more intuitive and easier to interpret compared to raw DN (Digital Number) values.\nImproved Machine Learning: Machine learning algorithms often perform better with scaled data. By converting to surface reflectance, we enhance the compatibility and effectiveness of these algorithms.\nEnhanced visualisation: Surface reflectance values are visually appealing and easier to visualise, aiding in data exploration and result communication.\nBy performing the scaling process, we ensure that the data is appropriately transformed and ready for further analysis.\n\n# create a mask for all the nodata using the first band\nmask = np.ma.masked_equal(l5_pc,raster_pc[0].nodata)\n\n\n#  Create an empty numpy array to store the scaled values\nl5_scaled = np.zeros(shape= l5_pc.shape, dtype= rasterio.float64)\n\n\nmult = 0.0000275 # scale\nadd = -0.2 # offset\n\n\n# now that we have same mask for every band, we can directly scale\n# we use np.where to restrict operations on non-masked elements\n# at the same time we convert the zero's no NaN since anyway the scaled data will be in float64\n\nl5_scaled =  np.where(~mask.mask,l5_pc* mult + add, np.nan)\n\n# check the data range\nprint((np.nanmin(l5_scaled), np.nanmax(l5_scaled)))\n\n(-0.0774325, 1.6022125)\n\n\n\n# check the size after\n# size will increase since the dtype is float64\n\nprint(f\"data type and the size before conversion is: {l5_pc.dtype}, {round(l5_pc.nbytes/1024**3,2)} GB\")\nprint(f\"data type and the size after conversion is: {l5_scaled.dtype}, {round(l5_scaled.nbytes/1024**3,2)} GB\")      \n\n\ndata type and the size before conversion is: uint16, 0.6 GB\ndata type and the size after conversion is: float64, 2.41 GB\n\n\nWe notice a significant increase in the image size from 0.6 GB to 2.41 GB. This enlargement is a consequence of converting the data from a 16-bit integer to a 64-bit float format, providing us with more precise decimal points.\nNow that we have our image stacked, scaled, and the nodata values replaced with NaN, it’s time to save it for future use. To accomplish this task, we turn to the powerful rasterio library, which offers efficient functions specifically designed for handling raster data. With rasterio, we can effortlessly save our processed image to our preferred device, ensuring easy accessibility for subsequent analysis and visualisation.\nUsing rasterio, we can save the processed image to our desired location, ensuring that it is readily available for further analysis and visualisation when needed.\n\n\noutput_l5 = './data/145044_20090425_ms_pc.tif'\n# we change the data to float64 since we scaled it\n\nprofile = raster_pc[1].profile\nprofile.update({\n    'count':6,\n    'dtype': rasterio.float64,\n    'nodata': np.nan\n})\n\n# keep in mind - compression is `deflate`\n# hence reducing the image size ~2.5GB gets reduced to ~300MB.\n\nwith rasterio.open(output_l5, 'w', **profile) as dst:\n    dst.write(l5_scaled)"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#read-the-data",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#read-the-data",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Read the data",
    "text": "Read the data\nTo begin, we read the training points from a geojson file using the geopandas library. These points were manually annotated by visually inspecting the image. Our annotation process involved creating a combination of NIR, Red, and Green bands, which allows us to observe burned areas as purple in color. Admittedly, identifying pixel classes at a 30m resolution can be challenging. However, with some remote sensing knowledge and training, it becomes feasible.\nI would like to express my gratitude to my supervisor, Dr. Meghna, whose decades of experience in studying the area and expertise in fires have been invaluable. Her knowledge has guided us throughout this project. I would also like to thank my co-RA, Wajida, who has been a real saviour in the annotation process. She has an exceptional talent for interpreting raw satellite images, and her contributions have been truly outstanding. A big shout-out to her!\nIn our research paper, we had approximately 12 classes, including forest, burned forest, agriculture, burned agriculture, fallen leaves, agriculture with no crop, water, shadows, and more. The aim of having these diverse classes was to differentiate between burned areas in forests and agriculture while also assessing potential confounding factors related to fire. This approach helps us build a more robust model. However, for the sake of simplicity in our current analysis, we have reclassified them into just two classes.\n\nVector Data\nFor handling the vector data, we leverage the geopandas library. By reading the geojson file, we can observe that the data is structured in geojson format, consisting of two columns: geometry and class. The geometry column contains the coordinates of the points, while the class column denotes the corresponding class for each point. Notably, we observe that there are two classes: Burned and Unburned.\n\ntraining_path = './data/BA_training_pixels_2.geojson'\ntraining_pixels = gpd.read_file(training_path)\ntraining_pixels.head()\n\n\n\n\n\n\n\n\nclass\nland_class\ngeometry\n\n\n\n\n0\n13.0\nBurned\nPOINT (739290.000 2659620.000)\n\n\n1\n1.0\nUnburned\nPOINT (730050.000 2650620.000)\n\n\n2\n13.0\nBurned\nPOINT (762240.000 2649120.000)\n\n\n3\n5.0\nBurned\nPOINT (798120.000 2648130.000)\n\n\n4\n4.0\nUnburned\nPOINT (781020.000 2646150.000)\n\n\n\n\n\n\n\nLet’s take a look at the distribution of the classes.\n\ntraining_pixels['land_class'].value_counts()\n\nUnburned    447\nBurned      253\nName: land_class, dtype: int64\n\n\nCreate a dictionary with class labels for later use.\n\nclasses = np.unique(training_pixels['land_class'])\nclass_dict = dict(zip(list(classes), range(len(classes))))\n\nclass_dict\n\n{'Burned': 0, 'Unburned': 1}\n\n\n\n\nRaster data\nWe read the downloaded image using rasterio and check for the projection match with the vector data. It is important to have both the training points and the raster image to same projection since we’ll overlay the points on the raster to extract the band values to build the training data.\n\n\nraster_path = './data/145044_20090425_ms_pc.tif'\nraster = rasterio.open(raster_path)\nbands = raster.read()\n\n# check if the both files are in same crs\nassert training_pixels.crs == raster.crs , \"crs are not the same\""
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#visualise-the-data",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#visualise-the-data",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Visualise the data",
    "text": "Visualise the data\nWe visualise the data by overlaying the points on top of the raster for a visual inspection. Taking a closer look at the histogram, we notice a significant number of outliers present on the edges. To address this, we apply a 2% stretching technique to the raster. By doing so, we enhance the contrast and improve the overall visual representation, allowing us to better discern the details within the image.\n\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\nfig, ax = plt.subplots(figsize = (10,10))\n\n# create the rgb bands\nrgb  = np.stack((bands[4], bands[3], bands[2]), axis =0)\n\n# Apply contrast stretching\n# get the 2, 98 percentiles for clipping\n\np2, p98 = np.nanpercentile(rgb, (2, 98))\n\n# clip the values within the range\n# every value below 2nd percentile and above 98th percentile will be truncated to the given percentile values\n\nrgb = np.clip(rgb, p2, p98)\n\n# apply the normalisation  (0-1) \nrgb = (rgb - p2) / (p98 - p2)\n\n\n# plot the image\ntraining_pixels.plot(ax =ax, color = 'red', alpha = .8, markersize = 8, marker = '+')\n\nshow(rgb, ax = ax, transform=raster.transform)\nplt.show()"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#data-prep",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#data-prep",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Data Prep",
    "text": "Data Prep\nTo create our training data, we need to gather the band values for each point in our labelled training samples and raster image. We do this by iterating through the points while traversing the raster and saving the values of each band into an array as rows.\nWe use the latitude and longitude values from the points as an index to locate the corresponding pixel position and its respective row and column position in our data arrays. Once we have this information, we extract the values for each pixel across all the bands.\nIf your labelled training samples are not points, but rather polygons or other shapes, you can rasterize them using the rasterio.rasterize() function and extract the band values for each polygon. More information on this process can be found in the rasterio documentation.\nWe also use the label values from each point to add them to the respective label dataset. This ensures that we have the appropriate labels assigned to their corresponding data points.\n\nband_vals =[] \nlabels = []\n\nfor index, row in training_pixels.iterrows():\n    x = row['geometry'].x\n    y = row['geometry'].y\n    label = row['land_class']\n    \n    # get the respective row, col\n    row, col = raster.index(x,y)\n\n    #get the data for all bands for that pixel\n    data = bands[:, row, col]\n\n    # add it to the X\n    band_vals.append(data)\n    \n    # add the respective label class to y\n    labels.append(class_dict[label])\n\n\n# convert band_vals and labels to full array \n\nX = np.array(band_vals)\n\ny = np.array(labels)\n\n# check if they have same length\n(X.shape, y.shape)\n\n((700, 6), (700,))\n\n\nWe can observe that the our new data has 700 rows and 6 cols. These 6 cols are the bands values for each point Since we have some training data outside the image, we should remove. This is important since the algorithm won’t run with NaN values.\n\n\n\n# mask values for the rows with nan\nnan_mask = np.any(np.isnan(X), axis =1)\n\n# remove the rows with the mask =TRUE\nX = X[~nan_mask]\n\n# apply the same for y\ny= y[~nan_mask]\n\n(X.shape, y.shape)\n\n((685, 6), (685,))"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#indices",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#indices",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Indices",
    "text": "Indices\nIndices are mathematical combinations of bands that are used to highlight a specific feature in the image.\nOne might ask, why do we need indices if already have those bands in the dataset. Answer is that, indices provide more information and highlight a specific characteristic of a pixel like vegetation,water etc. Thus, providing the algorithm with more patterns leading to improved results.\nWe add following indices to our dataset:\n\n\n\n\n\n\n\n\nIndice\nDescription\nLandsat 5 Calculation\n\n\n\n\nNDVI (Normalised Difference Vegetation Index)\nMeasure of vegetation and it is calculated as the difference between NIR and Red band. This is helpful since the burned area lacks vegetation.\n(Band 4 - Band 3) / (Band 4 + Band 3)\n\n\nNDWI (Normalised Difference Water Index)\nMeasure of water and it is calculated as the difference between NIR and SWIR band. This is helpful since the burned area gets confused with water.\n(Band 5 - Band 3) / (Band 5 - Band 3)\n\n\nNDMI (Normalised Difference Moisture Index)\nMeasure of moisture and it is calculated as the difference between NIR and SWIR band. This is helpful since the burned area lacks moisture.\n(Band 5 - Band 4) / (Band 5 + Band 4)\n\n\nBAI (Burned Area Index)\nMeasure of burned area and it is calculated as the difference between NIR and SWIR band.\n1.0 / ((0.1 - Band 4)^2 + (0.06 - Band 5)^2)\n\n\nNBR (Normalised Burned Ratio)\nMeasure of burned area and it is calculated as the difference between NIR and SWIR band.\n(Band 4 - Band 7) / (Band 4 + Band 7)\n\n\n\nWe write function that take the arrays and provide an ouput value for each row in an array\n\n\n# we calculate and use np.expand to add a dimension so that i match with the X which is needed when we add it to the main data\n\ndef indice_calc(array_in, band1, band2):\n    return np.expand_dims((array_in[:,band1]- array_in[:,band2])/(array_in[:,band1]+ array_in[:,band2]), axis = 1)\n\n\nndvi = indice_calc(X,3,2)\nndwi = indice_calc(X,4,2)\nndmi = indice_calc(X, 4,3)\nnbr = indice_calc(X, 3,5)\n\nbai = np.expand_dims((1.0 / ((0.1 - X[:,3]) ** 2 + (0.06 - X[:,4]) ** 2)), axis =1)\n\n# add the indices to the X\nX = np.concatenate([X, ndvi, ndwi, ndmi, nbr, bai], axis =1)\n\nOur final training data looks like this this. Every row is a point and every col is a band or an indice value. We have 11 cols and 685 rows. The first 6 cols are the band values and the last 5 cols are the indices values. We will have a corresponding label for each row in the label array.\n\ngpd.GeoDataFrame(X).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n0\n0.194597\n0.162587\n0.114710\n0.282103\n0.341915\n0.263375\n0.421843\n0.497575\n0.095851\n0.034332\n8.878047\n\n\n1\n0.183350\n0.151917\n0.103737\n0.248635\n0.325855\n0.273330\n0.411205\n0.517042\n0.134415\n-0.047312\n10.779202\n\n\n2\n0.135280\n0.120293\n0.086110\n0.173890\n0.235545\n0.210823\n0.337615\n0.464582\n0.150586\n-0.096000\n27.566603\n\n\n3\n0.177575\n0.144685\n0.101400\n0.262082\n0.305038\n0.227542\n0.442064\n0.501030\n0.075742\n0.070544\n11.585591\n\n\n4\n0.191132\n0.158243\n0.113252\n0.247947\n0.306605\n0.259580\n0.372910\n0.460519\n0.105774\n-0.022920\n12.091535"
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#model-building",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#model-building",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Model building",
    "text": "Model building\nIn machine learning models we train on a subset of data and test on the remaining data. We use the train_test_split function from sklearn to split the data into train and test sets. We use 80% of the data for training and 20% for testing. We also use stratify option to make sure that the class distribution is same in both train and test sets.\n\nfrom sklearn.model_selection import train_test_split\n\n# we split 80:20 for training and test\n# we use stratified split based on the class for balances train and test data\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .2, random_state =42, stratify = y)\n\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((548, 11), (137, 11), (548,), (137,))\n\n\nSince we don’t have equal number of samples for each class we create a dictionary that specifies the relative class distribution for our data. This would helps the algorithm to reduce the bias. We obtain the class weights by calculating the reciprocal of the class counts. These weights will assign higher importance to minority classes during model training, helping to mitigate the effects of class imbalance.\n\nlabels, counts = np.unique(y_train, return_counts = True)\nclass_weight_dict = dict(zip(labels, 1/counts))\nclass_weight_dict\n\n{0: 0.005, 1: 0.0028735632183908046}\n\n\n\nTrain the model\nRandom Forest is a supervised classification algorithm that uses ensemble learning method for classification. Ensemble learning is a type of learning where you join different types of algorithms or same algorithm multiple times to form a more powerful prediction model. The random forest algorithm combines multiple algorithm of the same type i.e. multiple decision trees, resulting in a forest of trees, hence the name “Random Forest”. In general, the more trees in the forest the more robust the forest looks like. In the same way in the random forest classifier, the higher the number of trees in the forest gives the high accuracy results.\nIt uses decision trees to classify the data into pre-defined classed and then utlise the majority voting to decide the final class. Some of the parameters are:\n\nNumber of Trees : Number of trees in the forest.\nNumber of Varibales per Split: Number of variables to consider when looking for the best way to split the data at each node of a decision tree.\nClass Weight: Weights associated with classes. If not given, all classes are supposed to have weight one.\n\n\n# build the model\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(\n    # number of decision trees\n    n_estimators=200,\n    # class weights\n    class_weight= class_weight_dict,\n    # n splits per tree\n    max_depth=2,\n    # parallelisation needed or not\n    n_jobs = 1,\n    # progress indicator\n    verbose =1,\n    random_state = 42\n)\n\n\n\nFit the model\n\n# fit the model\nclf.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.1s finished\n\n\nRandomForestClassifier(class_weight={0: 0.005, 1: 0.0028735632183908046},\n                       max_depth=2, n_estimators=200, n_jobs=1, random_state=42,\n                       verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(class_weight={0: 0.005, 1: 0.0028735632183908046},\n                       max_depth=2, n_estimators=200, n_jobs=1, random_state=42,\n                       verbose=1)\n\n\n\n\nPredict\nWe predict on test data using predict function and then calculate the accuracy score using accuracy_score function.\n\npreds = clf.predict(X_test)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n\n\n\n\nAccuracy Assement\nWe use the confusion matrix to assess the accuracy of the model. The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.\nThere is slight difference in the terms used for accuracy assessment in both remote sensing and machine learning. In remote sensing, we use the terms producer’s and user’s accuracy. In machine learning, we use the terms precision and recall. They are defined as follows:\n\nOverall accuracy: The proportion of correctly classified samples out of the total number of samples in the dataset.\nUser’s accuracy / Precision: The proportion of correctly classified samples for a given class out of the total number of samples predicted to be in that class.\nProducer’s accuracy / Recall: The proportion of correctly classified samples for a given class out of the total number of samples that are actually in that class.\n\nWe will also use F1 score to assess the accuracy of the model. F1 score is the harmonic mean of precision and recall. It is a good measure of model accuracy for imbalanced datasets. It is calculated as follows:\nF1 = 2 * (precision * recall) / (precision + recall)\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, preds)\n# normalised cm\ncm = cm.astype('float')/cm.sum(axis =1)[:, np.newaxis]\n\n# Plot confusion matrix as heatmap with class labels\nax = sns.heatmap(cm, annot=True, cmap='Blues', \n            xticklabels=class_dict.keys(), \n            yticklabels=class_dict.keys())\n\nax.set_title('Noramalised Confusion Matrix')\nax.set_xlabel(\"Predicted Label\")\nax.set_ylabel('True Label')\n\n\nsns.set()\n\n\n\n\n\n\n\n\n\n# calculate the overall accuracy\noverall_accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n\nprint(f'The overall accuracy is {overall_accuracy:.2f}.')\n\nThe overall accuracy is 0.81.\n\n\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Calculate precision for the burned class\nclass_precision = precision_score(y_test, preds, labels=[0], average=None)\nprint(f\"User's accuracy/Precision for Burned Class : {class_precision[0]:.2f}\")\n\n# Calculate recall for the burned class\nclass_recall = recall_score(y_test, preds, labels=[0], average=None)\nprint(f\"Producer's accuracy/Recall for Burned class: {class_recall[0]:.2f}\")\n\n# Calculate F1 score for the burned class\nclass_f1_score = f1_score(y_test, preds, labels=[0], average=None)\nprint(f\"F1 score for Burned class: {class_f1_score[0]:.2f}\")\n\nUser's accuracy/Precision for Burned Class : 0.68\nProducer's accuracy/Recall for Burned class: 0.84\nF1 score for Burned class: 0.75\n\n\nAs we can see from the confusion matrix, the model is able to predict the burned class with 84% accuracy. The model is able to predict the non-burned class with 77% accuracy. The overall accuracy of the model is 81%. But it is important to note that the overall accuracy is not a good measure of model accuracy for imbalanced datasets. Hence, we use F1 score to assess the accuracy of the model."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#hyper-parameter-tuning",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#hyper-parameter-tuning",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Hyper parameter tuning",
    "text": "Hyper parameter tuning\nBefore we finalise our model, we need to find the best parameters for the model. This is a process of finding the best parameters for the model that gives the best accuracy is called hyper parameter tuning. We use GridSearchCV function from sklearn to find the best parameters for our model. We provide a range of values for each parameter and the GridSearchCV function will try all the combinations of the parameters and find the best parameters for our model. We use the best parameters to train our model and then use that to predict all over the image.\nGridSearchCV also allows you to cross-validate your parameters. Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. In k-fold cross-validation, you split the input data into k subsets of data (also known as folds). You then iterate over k ML models. In each iteration, you use one of the k subsets as the test set (also called the validation data) and the union of the other subsets as the training set. For example, suppose you have 5000 input samples. You split the data into 5 folds of 1000 samples each. You train the ML model on 4 of the 5 folds (4000 samples) and evaluate it on the remaining 1 fold (1000 samples). You repeat this process 5 times (each time with a different fold as the evaluation dataset) and average the accuracy scores obtained in all the 5 iterations to get a final score for the ML model.\nIn our case, we use the same parameters as before and provide a range of values for each parameter. We use 5-fold cross-validation to find the best parameters for our model. We use the best parameters to train our model and then use that to predict all over the image.\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, make_scorer\n\n# create a classifier object\n\nclf = RandomForestClassifier(random_state=10, class_weight=class_weight_dict)\n\n# the parameter grid to be tested\n\nparam_grid = {\n    'n_estimators':list(range(100,500,100)),\n    'max_depth': [3,5,7],\n    # minimum numbee required to split another internal node\n    'min_samples_split': [2,4,6]\n}\n\n# perform grid-search with cross validation\n# we choose f1 score to as the metric to compare score\n\ngrid_search = GridSearchCV(clf, param_grid,cv = 5, scoring='f1')\ngrid_search.fit(X_train, y_train)\n\n# extract the best parameters and best score\n\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Parameters:\", best_params)\nprint(\"Best F1 Score:\", best_score)\n\n# we create a new clf with best params\nfinal_clf = RandomForestClassifier(random_state=10, class_weight=class_weight_dict, **best_params)\n\n# fit the model with the best params\nfinal_clf.fit(X_train, y_train)\n\nBest Parameters: {'max_depth': 7, 'min_samples_split': 2, 'n_estimators': 200}\nBest F1 Score: 0.8899791686173177\n\n\nWe can see that the F1 score has improved from 0.86 to 0.87. We can also see that the best parameters for the model are max_depth = 7, min_samples_split = 6 and n_estimators = 100. We use these parameters to train our model and then use that to predict all over the image."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#predict-over-the-entire-image",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#predict-over-the-entire-image",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Predict over the entire image",
    "text": "Predict over the entire image\nWe have built our model based on the training data from the full image which is a small portion. Now we will use the model to predict the burned area for the entire image. We will use the predict function to predict the burned area for the entire image. We are going by small patches to minimise memory use and to avoid crashes. For, that we read our full image and the collect the profile data to write the metadata for the classified band. We read the bands later by specifying a small patch of the image using window function. Parallel to this we open the image to saved in the write mode and then write onto it window by window.\n\nimport datetime\n\nstart_time = datetime.datetime.now()\n\n# reading raster windows\n# input raster\nl5_image  = raster_path\n\n# output file\noutput_image = './data/l5_ms_pc_classified.tif'\n\n# open the full image using rasterio\n\n\n\nwith rasterio.open(l5_image, 'r') as src:\n    profile = src.profile\n    #update profile for the classification ouput\n    profile.update(\n        # since we have nan\n        dtype = rasterio.float32,\n        # specify the band n\n        count = 1\n    )\n    # open the output file in write mode\n    with rasterio.open(output_image, 'w', **profile) as dst:\n\n        # specify the patch size\n        patch_size = 1000\n        \n        rows = src.shape[0]//patch_size+1\n        cols = src.shape[1]//patch_size+1\n        #rows = 2\n        #cols =2 \n        for i in range(rows):\n            for j in range(cols):\n                window = rasterio.windows.Window(\n                    row_off = i * patch_size,\n                    col_off = j * patch_size,\n                    # adding to ensure this doesn't go out of the image\n                    height = min(patch_size, src.shape[0] - i* patch_size),\n                    width = min(patch_size, src.shape[1] - j*patch_size)\n                )\n                #print(f\"Current window is {window.row_off, window.col_off}\")\n\n\n                # read the window\n                data = src.read(window=window)\n                \n\n                # we swap the axis since it get's the masking right\n                img_swp = np.moveaxis(data, 0,-1)\n                # now we flatten the data to 2-dim array for training\n                # -1 will infer the size of first dimension based on second dim, then band size would be used as a the second dim\n                img_flat = img_swp.reshape(-1, img_swp.shape[-1]) \n                # let's use indice_cal function to calculate indices\n\n                ndvi = indice_calc(img_flat,3,2)\n                ndwi = indice_calc(img_flat,4,2)\n                ndmi = indice_calc(img_flat, 4,3)\n                nbr = indice_calc(img_flat, 3,5)\n                bai = np.expand_dims((1.0 / ((0.1 - img_flat[:,3]) ** 2 + (0.06 - img_flat[:,4]) ** 2)), axis =1)\n                # add it to flat data using concat\n                img_w_ind = np.concatenate([img_flat,ndvi, ndwi,ndmi, nbr, bai], axis = 1)\n\n                # let's mask the invalid data before training\n\n                mask= np.ma.masked_invalid(img_w_ind)\n\n                # we apply to the data and extract only valid values\n\n                img_masked = img_w_ind[~mask.mask]\n\n                # since it converts the 2d to 1d, we need convert it back again\n\n                to_predict = img_masked.reshape(-1, img_w_ind.shape[-1] )\n\n                # some inputs will be empty especially if you have masked large portions\n                # we add a contingency option to skip that is the case\n\n                if not len(to_predict):\n                    continue\n\n                # predict\n                # returns a 1dim array with the predicted classes\n                img_pred = final_clf.predict(to_predict)\n\n                # add the data back to the valid pixels using first bands's mask as an index\n                # initiate an empty array to store the modified values after masking\n\n                output = np.zeros(img_flat.shape[0])\n\n                output[~mask.mask[:,0]] = img_pred\n\n                # convert to original image dimensions\n                # keep in mind that is only 1 band. so we need to give the row,col as an input\n\n                output = output.reshape(*img_swp.shape[:-1])\n\n                # create a final mask for writing\n                # so we convert the current mask to image dimensions but only one band\n                mask_w = (~mask.mask[:,0].reshape(*img_swp.shape[:-1]))\n\n                #write final files\n\n            \n\n                dst.write(output.astype(rasterio.uint8),1, window = window)\n\n                dst.write_mask(mask_w, window = window)\n\nend_time= datetime.datetime.now()\nprint(f\"Process finished. Time taken: {end_time-start_time}\")\n\n        \n\n\nProcess finished. Time taken: 0:03:58.447337\n\n\n\nVisualise the results\n\n# visualise the image\n\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nfig,(ax1, ax2) = plt.subplots(ncols =2, nrows= 1,figsize = (10,4), sharey=True)\n\n\n# read the classified image\nwith rasterio.open('./data/l5_ms_pc_classified.tif', 'r') as output_raster:\n    #show(output_raster)\n    classes = output_raster.read()\n\nshow(rgb, transform=raster.transform, ax = ax1)\nshow(classes, transform= output_raster.transform, ax = ax2)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that all the red areas are fire instances. Let’s quantify the percentage of area burned in the image.\n\n\nQuantify burned area\n\n# we use the classes variable to compute\n\n# count the number of pixels with 0 value excluding NaN\nclass_0_count = np.count_nonzero(classes == 0)\nclass_0_count\n\n# total pixels\ntotal_pixels = np.count_nonzero(~np.isnan(classes))\ntotal_pixels\n\nclass_0_pc = (class_0_count/total_pixels)*100\n\nprint(f\"total area of burned area in this image is: {class_0_pc:.2f}%\")\n\n# calculate in km2\nwidth_km = 170\nheight_km = 183\nl5_area_km2 = width_km * height_km\nl5_burned_area = l5_area_km2*(class_0_pc/100)\n\nprint(f\"total area of burned area in this image is: {l5_burned_area:.2f} KMsq\")\n\n\ntotal area of burned area in this image is: 52.74%\ntotal area of burned area in this image is: 16407.51 KMsq\n\n\nWe can observe that half the image area is burned and that is around ~ 16000 Kilometer Square Area."
  },
  {
    "objectID": "posts/ml-burned-area-prediction-landsat5-python/index.html#conclusion",
    "href": "posts/ml-burned-area-prediction-landsat5-python/index.html#conclusion",
    "title": "Predicting Burned Areas on Landsat 5 Images using Machine Learning in Python: A Comprehensive Guide",
    "section": "Conclusion",
    "text": "Conclusion\nWe have successfully built a model and use it to predict over the image. This shows that we can use machine learning to predict fire over large areas. We can use this model to predict fire over large areas and then use that information to take preventive measures.\nThere are many ways to improve the model. We can use more samples for burned class to improve the accuracy of the model. We can also use other algorithms like Support Vector Machine to improve the accuracy of the model. We can also use other informaton like slope, aspect, elevation, etc. to improve the accuracy of the model. I have limited the scope of this tutorial to Random Forest Classifier with limited data. But, you can try other algorithms and see if you can improve the accuracy of the model.\nYou can find the full notbeook and the data in this github repository: Link\nNote: Feel free to ask any questions if you need further clarification or have doubts!"
  },
  {
    "objectID": "posts/d3s/index.html",
    "href": "posts/d3s/index.html",
    "title": "Data Science Slides",
    "section": "",
    "text": "During the spring of 2023, I had the incredible opportunity to contribute as a teaching fellow for an undergraduate course at Ashoka University. The course, titled “Data Science for Social Science,” was primarily aimed at students studying Political Science and Economics. As part of the teaching team, my responsibilities included leading weekly discussion sections, where I provided in-depth reviews of the data science methods covered in the main lectures.\nIn these discussion sections, my approach involved a combination of practical examples, live coding demonstrations, and addressing students’ queries and challenges with their coding tasks. Each week, I conducted six discussion sections, with an average of 15 students attending each session. Overall, the class consisted of approximately 90 students.\nTo showcase my contributions and provide insight into my teaching approach, I have compiled a selection of slides from my discussion sections. These slides capture the essence of the topics covered and the interactive learning environment fostered during the course.\nFeel free to explore the following slides, which offer a glimpse into my instructional techniques and the impact I had on the students’ understanding of data science in the context of social sciences.\nIntroduction to R and R Studio\nData Manipulation with dplyr\nMerging Data\nData Visualisation using ggplot2\nPredicting Housing Prices using Linear Regression\nPython Basics"
  },
  {
    "objectID": "posts/cpr-pol-connect/index.html",
    "href": "posts/cpr-pol-connect/index.html",
    "title": "Data Science works at CPR",
    "section": "",
    "text": "In this blog you will find some of the data science work that I was involved in as an analyst at the Centre for Policy Research, New Delhi\nVisit the showcase website here"
  },
  {
    "objectID": "posts/gee-urbanisation-ml-sentinel-2/index.html",
    "href": "posts/gee-urbanisation-ml-sentinel-2/index.html",
    "title": "Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine",
    "section": "",
    "text": "In this post, I am sharing a project I completed as part of my machine learning course at Ashoka University. The objective was to leverage machine learning to understand the extent of urbanisation in the city of Delhi. Using the concept of built-up areas as a proxy for urban areas, I applied Random Forest machine learning algorithm to estimate the built-up area in Delhi."
  },
  {
    "objectID": "posts/gee-urbanisation-ml-sentinel-2/index.html#introduction",
    "href": "posts/gee-urbanisation-ml-sentinel-2/index.html#introduction",
    "title": "Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine",
    "section": "",
    "text": "In this post, I am sharing a project I completed as part of my machine learning course at Ashoka University. The objective was to leverage machine learning to understand the extent of urbanisation in the city of Delhi. Using the concept of built-up areas as a proxy for urban areas, I applied Random Forest machine learning algorithm to estimate the built-up area in Delhi."
  },
  {
    "objectID": "posts/gee-urbanisation-ml-sentinel-2/index.html#methodology",
    "href": "posts/gee-urbanisation-ml-sentinel-2/index.html#methodology",
    "title": "Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine",
    "section": "Methodology:",
    "text": "Methodology:\n\nObtaining Delhi Boundary Geometry:\nTo initiate the project, I obtained the precise boundary geometry of Delhi using the FAO/GAUL/2015/level2 dataset. This enabled me to focus specifically on the area of interest and streamline the analysis.\n\nvar delhi = ee.FeatureCollection(\"FAO/GAUL/2015/level2\").filter(ee.Filter.eq('ADM2_NAME','Delhi')).geometry();\n\n\n\n\nFiltering Sentinel-2 Imagery:\nNext, I accessed the Sentinel-2 satellite imagery from the COPERNICUS/S2_SR image collection. To ensure high-quality data, I filtered the imagery based on a cloud cover percentage of less than 30% and a date range of January 1, 2019, to December 31, 2019. Additionally, I restricted the imagery to be within the bounds of Delhi, using the previously acquired boundary geometry. Subsequently, I selected the relevant spectral bands (B4, B3, B2) for the analysis.\n// Code snippet 1: Filtering Sentinel-2 imagery\nvar sentinel2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n  .filterBounds(delhi)\n  .filterDate(\"2019-01-01\", \"2019-12-31\")\n  .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 30))\n  .select([\"B4\", \"B3\", \"B2\"]);\n\n\nCreating the Composite Image:\nAfterwards, I created a composite image by taking the median of the filtered images and clipped it to the boundary of Delhi. The resulting composite image provided a comprehensive representation of the land cover in the region.\nI then created a composite image by taking the median of the filtered images and clipped it to the boundary of Delhi. This composite image provided a comprehensive representation of the land cover in the region.\n// Code snippet 2: Creating the composite image\nvar composite = sentinel2.median().clip(delhi);\n\n\nVisualising the Composite Image\nTo visualise the composite image, I applied a colour visualisation scheme using the red, green, and blue bands. This enhanced the image and provided valuable insights into the distribution of different land cover types within Delhi.\n\n// Code Snippet: Visualising the Composite Image\n\n// Apply colour visualization parameters\nvar visualizationParams = {\n  bands: ['B4', 'B3', 'B2'],\n  min: 0.0,\n  max: 3000\n};\n\n// Display the composite image\n\nMap.addLayer(compositeImage, visualizationParams, 'Composite Image');\nComposite Image\n\n\n\n\n\n\n\nComposite image of Delhi\n\n\n\n\nLand Cover Classification Training Data:\nI manually annotated approximately 500 urban and non-urban points across Delhi to create the training data. I uploaded them to GEE assets as feature collection. I utilised that feature collection consisting of urban and non-urban points. Merging these collections provided a comprehensive set of training data for the classification process.\n// Code Snippet: Utilising the Feature Collection for Classification\n// Load the feature collections of urban and non-urban points\nvar urbanPoints = ee.FeatureCollection(\"projects/ee-najah/assets/urban_points\");\nvar nonUrbanPoints = ee.FeatureCollection(\"projects/ee-najah/assets/non_urban_points\");\n\n// Merge the urban and non-urban points collections\nvar trainingData = urbanPoints.merge(nonUrbanPoints);\n\n\nOverlaying Training Points and Extracting Training Data:\nNext, I overlayed the training points on the composite image to extract the necessary training data. This involved sampling regions within a specified scale of 10 units using the “sampleRegions” function, which resulted in the training data consisting of land cover labels associated with each region.\n// Code Snippet: Overlaying Training Points on the Composite Image\n// Sample regions within a specified scale of 10 units\nvar trainingData = compositeImage.sampleRegions({\n  collection: trainingData,\n  scale: 10,\n  properties: ['land_cover'],\n});\n\n\nSplitting the Dataset:\nTo evaluate the accuracy of the classification model, I split the dataset into training and testing sets. The training set was used to train the machine learning model, while the testing set was utilized to assess the model’s performance.\n\n\n// Code snippet 5: Splitting the dataset into training and testing sets\nvar split = 0.8; // 80% for training, 20% for testing\n\nvar trainingData = trainingData.randomColumn('split')\nvar training = trainingData.filter(ee.Filter.lt('split', split));\nvar testing = trainingData.filter(ee.Filter.gte('split', split));\n\n\nTraining the Random Forest Classifier:\nUsing the training data, I trained a random forest classifier with the “smileRandomForest” algorithm provided by Earth Engine. The classifier was trained with 50 trees, and the land cover property was used as the target class. The input properties for classification were derived from the band names of the composite image.\n// Code Snippet: Training the Random Forest Classifier\n\n// Train a random forest classifier with 500 trees\nvar classifier = ee.Classifier.smileRandomForest(500)\n  .train({\n    features: training,\n    classProperty: 'land_cover',\n    inputProperties: ['B4', 'B3', 'B2']\n  });\n\n\nApplying the Classifier to Generate Land Cover Classification Map\nFinally, I applied the trained classifier to the composite image to generate a land cover classification map. The classified image assigned distinct colours to different land cover classes, with the colour palette including shades of grey, brown, blue, and green. This visualisation provided a clear understanding of the land cover distribution in Delhi during the year 2019.\n// Code Snippet: Applying the Classifier to Generate Land Cover Classification Map\n\n// Apply the trained classifier to the composite image\nvar classified = compositeImage.classify(classifier);\n\n// Display the land cover classification map\nMap.addLayer(classified, { palette: ['gray', 'red'] }, 'Land Cover Classification');\nLand Classification Map\n\n\n\n\n\n\n\nImage: Land cover classification map of Delhi\n\n\n\n\nAccuracy Assessment:\nTo assess the accuracy of the classification model, I calculated the confusion matrix using the testing dataset. The confusion matrix provided insights into the model’s performance, including metrics such as overall accuracy, producer’s accuracy, and user’s accuracy.\n// Code snippet 7: Calculating the confusion matrix for accuracy assessment\nvar testAccuracy = testing\n    .classify(classifier)\n    .errorMatrix('class', 'classification');\n\nprint('Confusion Matrix:', testAccuracy);\nprint('Overall Accuracy:', testAccuracy.accuracy());\n\n\nCalculating Urban Area Percentage:\nTo understand the extent of urbanization, we calculated the area covered by the urban class within Delhi. We also calculated the total area of Delhi to determine the percentage of urban area.\n\n// New Code Snippet: Calculating urban area percentage\n\n// Calculating area of urban class\nvar urban = classified.select('classification').eq(1);\n\n// Calculate the pixel area in square kilometer for urban\nvar area_urban = urban.multiply(ee.Image.pixelArea()).divide(1000 * 1000);\n\n// Reducing the statistics for urban area in Delhi\nvar stat_urban = area_urban.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: delhi,\n  scale: 100,\n  maxPixels: 1e10\n});\n\n// Get the sum of pixel values representing urban area\nvar area_urban_sum = ee.Number(stat_urban.get('classification'));\n\n// Calculate the pixel area in square kilometer for Delhi\nvar area_delhi = ee.Image.pixelArea().clip(delhi).divide(1000 * 1000);\n\n// Reducing the statistics for Delhi\nvar stat_delhi = area_delhi.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: delhi,\n  scale: 100,\n  maxPixels: 1e10\n});\n\n// Get the sum of pixel values representing Delhi area\nvar area_delhi_sum = ee.Number(stat_delhi.get('area'));\n\n// Calculate the percentage of urban area\nvar percentage_urban_area = area_urban_sum.divide(area_delhi_sum).multiply(100);\n\n// Print the urban area in square kilometers\nprint('Urban Area (in sq.km):', area_urban_sum);\n\n// Print the total area of Delhi\nprint('Delhi Area (in sq.km):', area_delhi_sum);\n\n// Print the percentage of urban area\nprint('Percentage of urban area:', percentage_urban_area);\n\n\nExporting the Results:\nLastly, I exported the classified image and the confusion matrix results to Google Drive for further analysis and sharing with collaborators.\n\n// Code snippet 8: Exporting the classified image and confusion matrix to Google Drive\nExport.image.toDrive({\n  image: classified,\n  description: 'classified_image',\n  folder: 'GEE_Classification_Results',\n  scale: 10,\n  region: delhi\n});\n\nExport.table.toDrive({\n  collection: testAccuracy,\n  description: 'confusion_matrix',\n  folder: 'GEE_Classification_Results',\n  fileFormat: 'CSV'\n});"
  },
  {
    "objectID": "posts/gee-urbanisation-ml-sentinel-2/index.html#results-and-conclusion",
    "href": "posts/gee-urbanisation-ml-sentinel-2/index.html#results-and-conclusion",
    "title": "Estimating Urbanisation using Machine Learning and Sentinel-2 Imagery in Google Earth Engine",
    "section": "Results and Conclusion",
    "text": "Results and Conclusion\n\nThe land cover classification analysis using the basic Random Forest model achieved an overall accuracy of 87%, indicating the model’s effectiveness in accurately classifying land cover types in Delhi based on satellite imagery data.\nTo further improve the classification results, additional enhancements can be implemented, such as incorporating spectral indices like NDVI (Normalised Difference Vegetation Index) or NDBI (Normalised Difference Built-up Index) to provide additional information for distinguishing different land cover types.\nHyperparameter optimisation can be performed to fine-tune the Random Forest model, adjusting parameters such as the number of trees, tree depth, and feature subsampling, with the potential to achieve higher accuracy.\nThis project primarily focused on land cover classification in Delhi, but there is potential for extending the analysis to other Indian cities. Analysing the model’s performance in different urban environments would provide valuable insights into its generalisability and scalability.\nThe classification results suggest that approximately 78% of Delhi can be classified as urban, highlighting the significant urbanisation within the city.\nBy tracking the historical growth of urban areas in Delhi using satellite imagery, it is possible to gain a deeper understanding of urban expansion patterns over time.\n\nLink to Google Earth Engine script\nLink to original research paper"
  },
  {
    "objectID": "posts/compare-spectra/index.html",
    "href": "posts/compare-spectra/index.html",
    "title": "Compare Spectra App: A Simple and User-Friendly Tool for Spectral Analysis",
    "section": "",
    "text": "TL;DR:\nI’ve developed a new Earth Engine app for spectral analysis. It’s simple, user-friendly, and free to use. Check it out!\nI am excited to share a new Earth Engine app that I’ve developed, designed to simplify spectral analysis for remote sensing projects. It’s called the compare-spectra app, and it’s a simple and user-friendly tool that makes it easy to explore and compare spectra without the need for extensive coding or expensive tools.\nThe compare-spectra app allows you to select your study area, draw regions of interest (ROIs), and generate comparative line charts that visualize the spectra of the selected areas. You can also download the generated charts as images or export the spectral data as a CSV file.\nThe app is a web-based application, so you can use it without the need to install any software. Simply open the app in your browser and follow the instructions.\nHere are some of the features of the compare-spectra app:\n\nSimple and user-friendly interface: The app is easy to use, even if you’re not familiar with Earth Engine.\nNo coding required: You don’t need to know any code to use the app. Cost-effective: The app is free to use.\nFlexible and customizable: You can customize the app to fit your specific needs.\nOpen source: The app is open source, so you can contribute to its development.\n\nIf you’re interested in trying out the compare-spectra app, you can visit the app link. I’m looking for feedback and suggestions on how to improve the app, so please feel free to leave your feedback in the comments section below.\nI hope you find the compare-spectra app useful. If you have any questions, please don’t hesitate to contact me.\nCheers!\nHere’s the demo on how to use the app :"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please use the form below to send me a message.\nI’ll try to get back to you as soon as possible.\n\n\nName \nEmail \nMessage\n\n\n\nSend"
  },
  {
    "objectID": "contact.html#get-in-touch",
    "href": "contact.html#get-in-touch",
    "title": "Contact",
    "section": "",
    "text": "Please use the form below to send me a message.\nI’ll try to get back to you as soon as possible.\n\n\nName \nEmail \nMessage\n\n\n\nSend"
  },
  {
    "objectID": "posts/media-contributions/index.html#this-page-showcases-selected-media-collaborations-under-the-banner-of-vertical52-and-features-where-my-satellite-data-and-geospatial-work-has-supported-public-understanding-of-conflict-urban-planning-and-disaster-impact.",
    "href": "posts/media-contributions/index.html#this-page-showcases-selected-media-collaborations-under-the-banner-of-vertical52-and-features-where-my-satellite-data-and-geospatial-work-has-supported-public-understanding-of-conflict-urban-planning-and-disaster-impact.",
    "title": "Media Contributions",
    "section": "",
    "text": "title: “Media Contributions” format: html: toc: true toc-depth: 2 number-sections: false page-layout: full theme: cosmo —"
  },
  {
    "objectID": "posts/media-contributions/index.html#conflict-crisis-damage-mapping",
    "href": "posts/media-contributions/index.html#conflict-crisis-damage-mapping",
    "title": "Media Contributions",
    "section": "",
    "text": "Created SAR-based proxy damage maps for Ukrainian cities.\n🔗 Tagesspiegel – Das unvorstellbare Ausmaß von zwei Jahren Ukraine-Invasion\n\n\n\nContributed visual and satellite-based analysis for assessing destruction in Mariupol.\n🔗 Süddeutsche Zeitung – Mariupol: Ukraine, Russland und der Krieg\n\n\n\nProduced damage maps and conducted visual verification for building destruction.\n🔗 Stern – Wie der Küstenstreifen systematisch zerstört wurde\n\n\n\nMaps and spatial analysis featured in retrospective media on one year since the Gaza war began.\n🔗 NZZ – One Year of Catastrophe: Three Voices from the Gaza Strip\n\n\n\nAssisted with satellite-based mapping of fire and explosion footprints during active conflict.\n🔗 Handelsblatt – Satellitenbilder zeigen den Krieg in Israel und Gaza\n\n\n\nSupported statistical and spatial visuals on the Gaza conflict.\n🔗 Süddeutsche Zeitung – Gaza: Zahlen des Kriegs\n\n\n\nGenerated estimates of damaged buildings and regions using remote sensing.\n🔗 NZZ – Turkey struggles to remove 200 million tons of quake debris"
  },
  {
    "objectID": "posts/media-contributions/index.html#urban-heat-environmental-monitoring",
    "href": "posts/media-contributions/index.html#urban-heat-environmental-monitoring",
    "title": "Media Contributions",
    "section": "",
    "text": "Built ML models for estimating sealed surfaces; featured in multiple regional reports.\n🔗 Correctiv – Versiegelung deutscher Städte nimmt zu\n🔗 NDR – Klima in Hamburg: Temperatur hängt vom Stadtteil ab\n\n\n\nMapped reconstruction progress and flood-affected areas using remote sensing.\n🔗 Süddeutsche Zeitung – Wiederaufbau im Ahrtal"
  },
  {
    "objectID": "posts/media-contributions/index.html#interactive-platforms",
    "href": "posts/media-contributions/index.html#interactive-platforms",
    "title": "Media Contributions",
    "section": "",
    "text": "Developed an interactive website for visualising Gaza building damage; full-stack delivery from layers to interface.\n🔗 Gaza Damage Map Portal\n\nSee more on my portfolio or GitHub."
  }
]